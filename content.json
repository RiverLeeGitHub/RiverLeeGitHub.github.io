{"pages":[],"posts":[{"title":"Kiwi!","text":"视频链接：KiWi! KIWI是一只无翼的小鸟，从小就有一个飞翔的梦想。为了实现自己毕生的“飞行”梦想而费劲千辛万苦将一棵棵树水平的钉在了陡峭的悬崖上，直延伸至山谷的最底部。就在最后一棵树被KIWI牢固的钉在峭壁上后，KIWI的准备工作全部结束了，它即将实现它的梦想——飞行。KIWI显得有些兴奋，它用掌声为自己鼓励加油，为了这次飞行它甚至还准备了飞行帽。 从山巅到绝谷，这是一次必死的飞行，为了飞行之梦KIWI显得从容而坚定。 一个漂亮的鱼跃后，KIWI一生中最辉煌的时刻在飞行中度过了，但这也是它最后的辉煌。风在耳边呼啸而过，KIWI的眼里悬崖成了地平线。令人激动的飞翔，白云在身边飘过，身下树木郁郁葱葱。KIWI努力的扇着自己那小的几乎毫无作用的翅膀，想像着自己是一只健康的小鸟，无拘无束地遨游在蓝天之下，那是它一生的梦想。KIWI眼角溢出了一滴泪水，混合着所有的酸甜苦辣。那是为梦想得以实现而泣,是为自己的命运而泣,是为那瞬间的幸福满足而泣,是为眷恋这即将离开的世界而泣。 世上什么最可贵？只有一次的生命最可贵，失去了永远不会再来。KIWI为了一生唯一的一次飞行梦想而义无反顾地付出生命，我们又有什么理由不去为自己的梦想而奋斗，至少我们还用不着为此而付出生命的代价。","link":"/2013/08/05/Kiwi/"},{"title":"MapReduce和YARN的区别和联系[转]","text":"初学Hadoop，多多学习 :) 本文转载自MapReduce和YARN的区别和联系 Hadoop 的最常见用法之一是 Web 搜索。虽然它不是唯一的软件框架应用程序，但作为一个并行数据处理引擎，它的表现非常突出。Hadoop 最有趣的方面之一是 Map and Reduce 流程，它受到Google开发的启发。这个流程称为创建索引，它将 Web爬行器检索到的文本 Web 页面作为输入，并且将这些页面上的单词的频率报告作为结果。然后可以在整个 Web 搜索过程中使用这个结果从已定义的搜索参数中识别内容。 MapReduce最简单的 MapReduce应用程序至少包含 3 个部分：一个 Map 函数、一个 Reduce 函数和一个 main 函数。main 函数将作业控制和文件输入/输出结合起来。在这点上，Hadoop 提供了大量的接口和抽象类，从而为 Hadoop应用程序开发人员提供许多工具，可用于调试和性能度量等。 MapReduce 本身就是用于并行处理大数据集的软件框架。MapReduce 的根源是函数性编程中的 map 和 reduce 函数。它由两个可能包含有许多实例（许多 Map 和 Reduce）的操作组成。Map 函数接受一组数据并将其转换为一个键/值对列表，输入域中的每个元素对应一个键/值对。Reduce 函数接受 Map 函数生成的列表，然后根据它们的键（为每个键生成一个键/值对）缩小键/值对列表。 这里提供一个示例，帮助您理解它。假设输入域是 one small step for man,one giant leap for mankind。在这个域上运行 Map 函数将得出以下的键/值对列表： （one,1） (small,1） (step,1） (for,1） (man,1） (one,1） (giant,1） (leap,1） (for,1） (mankind,1） 如果对这个键/值对列表应用 Reduce 函数，将得到以下一组键/值对： （one,2） (small,1） (step,1） (for,2） (man,1）（giant,1） (leap,1） (mankind,1） 结果是对输入域中的单词进行计数，这无疑对处理索引十分有用。但是，假设有两个输入域，第一个是 one small step for man，第二个是 one giant leap for mankind。您可以在每个域上执行 Map 函数和 Reduce 函数，然后将这两个键/值对列表应用到另一个 Reduce 函数，这时得到与前面一样的结果。换句话说，可以在输入域并行使用相同的操作，得到的结果是一样的，但速度更快。这便是 MapReduce 的威力；它的并行功能可在任意数量的系统上使用。 Hadoop它是如何实现这个功能的？一个代表客户机在单个主系统上启动的 MapReduce应用程序称为 JobTracker。类似于 NameNode，它是 Hadoop 集群中惟一负责控制 MapReduce应用程序的系统。在应用程序提交之后，将提供包含在 HDFS 中的输入和输出目录。JobTracker 使用文件块信息（物理量和位置）确定如何创建其他 TaskTracker 从属任务。MapReduce应用程序被复制到每个出现输入文件块的节点。将为特定节点上的每个文件块创建一个惟一的从属任务。每个 TaskTracker 将状态和完成信息报告给 JobTracker。 Hadoop 的这个特点非常重要，因为它并没有将存储移动到某个位置以供处理，而是将处理移动到存储。这通过根据集群中的节点数调节处理，因此支持高效的数据处理。 Hadoop是一种分布式数据和计算的框架。它很擅长存储大量的半结构化的数据集。数据可以随机存放，所以一个磁盘的失败并不会带来数据丢失。Hadoop也非常擅长分布式计算——快速地跨多台机器处理大型数据集合。 MapReduce是处理大量半结构化数据集合的编程模型。编程模型是一种处理并结构化特定问题的方式。例如，在一个关系数据库中，使用一种集合语言执行查询，如SQL。告诉语言想要的结果，并将它提交给系统来计算出如何产生计算。还可以用更传统的语言(C++，Java)，一步步地来解决问题。这是两种不同的编程模型，MapReduce就是另外一种。 MapReduce和Hadoop是相互独立的，实际上又能相互配合工作得很好。","link":"/2019/03/01/MapReduce和YARN的区别和联系-转/"},{"title":"我游离的归属感","text":"每到周末，从忙碌的学校挣脱出来，一个人呆在爸爸妈妈都去上班的空屋子里，我的内心总是空荡荡的，似乎少了些什么，但是又好像无法弥补。 还没有回味完早上的阳光，在恍惚间黄昏已在跟我招手。看着床上堆得成山的作业，我的心情有些失落，或者说是很糟糕，觉得自己不是一般的颓废，真是越大越不如从前了。我看了看电脑右下角的时间，才恍然现在已经是2013年了，这个数字安在我的身边，很陌生、很畏惧，但是又不得不面对。 也不知道为什么，在我的印象中，去年才刚刚汶川地震，北京举办完奥运会，过了几个月完成了世博，然后是大运会我们的庆祝，这些好像离我都没有远去，可是数数已经过了四五个年头，我来到世上三分之一的时光。 呵呵，刚刚痛哭流涕告别小学，现在初中的同学已经分离。虽然留了小学同学们的联系方式，可是这又有什么用呢，我们的隔膜已经很深很深了，好像再也找不到能够搭讪的理由。我知道，就算我们聊了起来，小学的记忆便被陌生的成熟的言语抹去，相见不如不见！ 回家的地铁站内遇到满满，我们在人群中多了那么一瞥，就这样尴尬地聊了几句就分别了，我不知道这是不是我人生中最后一次跟她的相遇。杨欣在QQ中跟我聊天，我才知道小学时她来我家碰到了的邻居邹涵今天是她的同班同学，我们当初又如何会想到这些！我们聊了一些近况，问了问她和嘎杂张瑞宗的班级，以一如既往的形式调侃了几句。知道了杨欣是语文课代表，张瑞宗在高一三班。可是这又能怎么样，我们终究不能回到过去。 爸爸妈妈南下闯荡的将近二十年中，我们已经在深圳六所房子里留下过生活的印记。对于爸爸妈妈来说，爷爷家和外公家是他们永恒的根；对于我这个跟父母生活在一起的人来说，我的根没有落脚点。 今年春节，我们从馨庭居搬走，来到凯旋城。馨庭居的余韵还留着：我记得胖子保安陪我从小玩到大，记得和家人邻居在花园里打羽毛球，记得爸爸跟方叔叔的聊天，记得和妈妈一起每天晚上吊单杠，记得早上响起的幼儿园清丽的歌声……我对这儿的记忆太多太多，可是此去经年终将被时光遗忘。告别了僻静的社区，我来到一个完全陌生的环境中，很忐忑，很想回去，可是旧房子已经租给别人，我发现我已经回不去那个地方了，我回不去了。 与初中部揖别，我来到仅一路之隔的高中部。高中生活还很崭新，但是已经非常熟悉，就像一个在宝中百般历练经验丰富的老者一样。从窗外总能望到我们曾经哭过笑过的地方，可是一切都变得可望不可即。我能看到原来屹立在三楼的七村，看到新修的运动场，看到曾经中午期待的饭堂，看到巍峨的湛蓝天亭，可是我看到初中部的围墙变得很高很高。 我知道我的恩师们正在以同样的心血去哺育后来者，我也知道我已经回不去那个地方了，我回不去了。 我还记得初中的时候，曾心妍跟我说高中的人很有心计，让我提防一些。今天我意识到高中没有想象的那么勾心斗角恐怖至极，只是有些人们的友谊之间多了很多与利益相关的东西，这样的友谊是虚伪空洞的，不像初中的那么真、那么纯。 不过高中生活里还是存在真情的。和宿友在月光下奋斗，调侃生活琐事，建立抗击宿管统一战线；和同僚互相尊重而谦让，小事中诚意如花；和挚友一起踏着夕阳回家，一路浩荡行进，驰骋云端；和死党勾肩搭背，互相激励成长。妈妈的爱化作短信的文字夜夜飘进我的梦乡，爸爸的问候充满着对我的期待与鼓励。有时候发现自己原来很幸福，我的心灵皈依到这浓浓情感上来了。 真的是逝者不舍昼夜啊，我们所共同拥有的时间就像浩浩江水滚滚远去。我们被水流冲向远方，不知道出发点在哪里、目的地在何处。看着河岸美丽的风景，想留下来，可是河流终究无情流走，带我们来到未知的前方。 有时候觉得好累，我们一天天成长，又一天天老去，为很多事情欣喜过、悲伤过，可是什么都只属于我的过去，再也找不出来了。我想在河流中找块石头倚傍，靠着它歇一歇，感觉漂得太快了，可是我发现我停不下来，身旁只有湍急的波澜。 在时间的洪流中，只有你们在，嬗变才是永恒。我们无依无靠，但期待能够携手同行。","link":"/2013/03/24/我游离的归属感/"},{"title":"星空杂想","text":"经常能够在电视上面看到有关宇宙星空之类的纪录片，经常入神于深邃而又飘渺的宇宙空间，看苍穹繁星点点，地球的蓝色光辉在太阳系中显得格外耀眼。 这是一颗拥有生命的星球，蓝色的海洋哺育了一代又一代的奇特物种。今日，人类作为蓝色星球的主宰者在翘首而观，在点点繁星中迷蒙了方向。 发展与利益相对于地球乃至宇宙的诞生，人类的出现是及其短暂的，人类又是何其得渺小。但是人类却已经征服了自然，在这颗蓝色星球上创造了无尽的文明。 文明，是厚重而悠远的。曾经多少文明的积淀如今却寥寥无几，留下的只是青铜器上几个刻画符号罢了，但是现今的人类依然能够从这些文物中感觉得到远古文明的深重影响。新石器时代以来，人类间出现了金钱的交易，古往今来，世世代代的祖先正是凭借金钱与利益发展而来。因为利益人们的技术开始迅速发展繁荣，因为利益人类文明才会有社会性质的改变。经过几千年的金钱交易，人们的财富梦想变质了。之前利益是发展的工具，现在发展却是利益的工具。要知道，一个人类文明存在的最终目的不是为了发展而是追求利益的满足，那么这个文明将不再会闪光。 文明的消失地球经历了种种时代的嬗变：三叶虫繁盛过，陨石一撞击便荡然无存；恐龙称霸过，同样性质的陨石一来也消失殆尽。今天轮到人类面对着这样的问题。我们是怎么知道这些亘古出现的物种的？因为它们留下了化石。 世事没有绝对，地球出现过就一定会消失，这只不过是时间的问题。那么地球消失后的外星生物一定不知道：在这个消逝了的蓝色星球中曾经出现了怎样繁华的人类文明！我们现在在幸福地过日子，等到地球消失的那一瞬间就什么也没有了，我们何曾想过留下文明的印记？人总归是要逝世的，看就看你能不能在人们的心中活得更久。如果后人将你的事迹广为传颂，激励了一代又一代后人，那么你的这一生就没有枉活。那么上天为什么要让人出世呢，因为你肩负着历史的重任，你要推动这个世界的发展。而一个文明的进步正是通过从古到今无数人民的贡献搭就的。 如果拥有这种为人类发展而做出贡献的精神，即使预言中的2012是真实的，那又如何呢？我们可以骄傲地告诉后来居住在这个星球中的人：你们的祖先好样的！ 明亮的夜晚我曾经做过一个梦。小时候，我梦见睡梦中的我忽然被喧闹的汽笛声惊醒，抬头观天，午夜的天空如白昼一样光亮。我欣喜地张开手臂，拥抱洁白的苍穹，旋转，在明亮中眩晕陶醉。 今夜，我抬头而望，头顶上的天空已经亮如白昼，远处与人类灯光的对比之中才黯然失色。我不希望将来的那么一天，我们只有白天而没有夜晚，只有红灯绿酒没有酣然醉梦。城市里的星星不再成片，只有一两颗零碎地点缀天空，傍着月亮唱着原始的摇篮曲。 从太空来看，夜晚的陆地是璀璨的。人类在这片沃土上创造了何其的光芒啊，从一点微弱的火花到世界的光辉，你们带给地球的是怎样的伤痕？ 宇宙中的点点繁星不语，看着这颗发展中的蓝色星球默默惆怅。","link":"/2012/02/09/星空杂想/"},{"title":"朦胧中秋夜","text":"好久没写篇像样的日志了！我在初中的很长一段时间基本上没有时间写这种东西，所以那些零星的琐事自然也不会放进空间。除非是那些堆积在心里实在是憋不住了才会写下来，所以其中通常缺乏一些平常的生活情趣。好不容易有个作业很少让我很激动的假期，我就冲破桎梏写写琐碎的流水账吧。 爸爸回到老家了。暮色四合，我和老妈晃晃悠悠地到舅妈家蹭饭，顺便看看学会了整数和为100以内加减法的鄙弟。宝安大道又一次挂上了耀眼的国旗，印象中跟去年的国庆好像之隔两个多星期而已。在他们家吃完饭后便堆在电视机前看《喜羊羊与灰太狼》，在家里好像总是搜不到喜羊羊的频道。紧接着出现了传说中的《黑猫警长》甚至是《葫芦兄弟》。鄙弟突然从后面伸出一只大脚来搭在我肩上，不知不觉便在我上面坐了上来。妈妈和舅妈便聊着那神奇的100以内算数法，手中不停比划着鄙弟幼儿园教的手指决。 天色已晚，回程的公交车灯昏暗。我透过窗户看着头顶的月亮又大又圆，有点沉醉。猛然想起什么，便叫老妈拨个老爸电话。“老爸，在干吗，中秋节快乐！”爸爸这时正准备上楼睡觉。农村爷爷家的双层小房子我一般都是冬天时才跟它见面的，脑子里瞬间想起了门前月光下的柚子树、想起晚上没灯我害怕叫老爸不停发出声音让我听到我才敢上楼、想起爷爷灶房的薪火四溅、想起我穿着大棉袄看着春晚趴在老妈怀里睡着了的情景…… 车停了，我和妈妈跳下车门，随即走上宝安大道的那座无顶天桥。曾经总是抱怨这座天桥怎么没个遮阳顶，今天反倒让我庆幸了。天桥下橘黄的路灯照亮着目的地的旅途，头上明亮圆润的月照亮着我们时光的航程。“哇！”天空中出现了忽明忽暗的火光，是孔明灯吧。我和妈妈便在天桥上驻足，等待着孔明灯消失在遥远的空中，我们欣喜地许愿。孔明灯在正东方向的珠江口便隐隐下降了，直至熄灭我们看不到了为止。我们看着带有光晕的月亮，发现这个月亮带着耳朵，莫非是“月珥”？妈妈说月亮像个小男孩的侧脸，我怎么看都不像啊，可是她却一意坚持。 我都无法面对花园的保安了：在我们花园张灯结彩挂上红灯笼的时候，他们却在保安亭守着监控录像遐思。我不敢在他们面前把自己表现地很快乐，因为我们的快乐会加剧他们的思念，便热情地打了个招呼：“节日快乐。”现在的天空基本上没有颗像样的星星了，在我年轻的时候还有一两颗点缀。妈妈突然指着天空叫我看，我看到了一个畏畏缩缩的星星朝着我勉强地笑。妈妈还看到几颗我都看不见的星星，我却并没有看到。她担心我的眼睛变近视，我便调侃道：“一般电视里精神病人都会看到正常人看不到的东西，还一个劲说：‘那里，就在那里’，正常人还会附和着：‘好好好，有的’”妈妈于是笑出了声，然后就装作生气地责怪我。 回家上了QQ，外面响着所谓《十五的月亮》的歌，这可是陪了我十几年的经典啊。我几乎跟每个孩子发了个“节日快乐！”从而揪出几个潜水很深的娃，探寻到正在煲电视粥的，发现有在KTV还问我来不来的，找到一个总是对我发哭脸问他怎么了竟然说他耍我的。粘贴“节日快乐！”贴到我手都抽筋，然后招来一大群回复，我基本上都会回个呲牙表情。而且最让我吃惊的是，孩子们的回复大多是异口同声出现“同乐”这个短语，让我不禁汗颜。发祝福，自己的心也热烘烘的。 这个夜晚虽然没有吃到月饼，但是我觉得比以往还要满足，因为我今夜感受到的爱是朦胧的。月亮这个东西只是个陪衬，月饼这个东西只是个形式，重要的是在中秋夜有着暖暖的爱意，这才有意义。","link":"/2012/09/30/朦胧中秋夜/"},{"title":"社工情","text":"在那个鲜花盛放的季节，在那个空灵澄澈的记忆里，与小学的时光牵连在一起的，是笑中含泪的社工情。 笑声清晰地记得，我还是一个五年级学生的时候。那一次是柔美的正午时光，宝城六班的人群熙熙攘攘地从体操室里走出来。听说学校里来了位社工，我就试探地走向三楼的那间小屋，那间令我之后永远魂牵梦萦的学校社工室。我记得杨欣也在的，我们跟一位很年轻的社工在温馨的房间里谈着有关父母、时间与爱之间的内容。她叫高汝虹，朦胧的名字。印象中，那个中午非常非常得温暖，我的心灵瞬间被爱包裹。 之后的时日，我、王琛（Mr.WC）还有杨欣、王肖童就经常把那间简陋的小屋当做第三个家。我们在那里无话不谈，回荡的只有我们的天真的笑。放学之后，我爱和WC一起在社工室写作业，高社工屡次善意地劝阻我们不要在那里写作业，我们便乖乖地收起来，玩着柜子里摆放的五子棋。WC与我对膝而坐，在黑白棋子中行走着我们的命运。杨欣喜欢在这小屋里跑来跑去，爱在充满绿意的沙发上肆意翻滚。我们在小屋里玩着春意阑珊，似真似幻的捉迷藏。社工室门上原来贴了一张有许多动物的图片，其中一幅是一只小狗咧着牙放纵的笑脸。我和王肖童只要一看见他便会忍俊不禁，笑到日暮黄昏。在那一方净土中，留下的只有欢乐。 我还记得那一天放学，我和WC意外地发现社工室锁着门。我们侧耳倾听，听见里面高社工在对几个同学讲环境保护。我们便从楼下捡来一片绿油油的树叶，用笔写到“免费讲座”云者，塞进门缝里再敲门。门一打开叶书便飘落于地，高社工拾起，既诡疑又含笑，其人乐哉。还有一次，高社工组织活动，利用废旧纸张制作储蓄罐。我和WC用饮料瓶制作了一个类似于火箭的罐子，天真的我们还以为有人会偷里面装的硬币，于是设计了许多隐蔽的装置，取名为“李琛牌小罐罐”。 那一天小学毕业，那是我最后一次以小学生的身份拜访高社工了。心中无尽的留恋，心有无尽的苦水，我需要与高社工作最后的告别。我不幸地看见门是关着的，我以为高社工今天又外出了，我们连最后一面都见不上了。我忐忑地敲了一下门，我怀着一种绝望的心情无力地敲门，希望有奇迹闪现，眼中流下痛苦的泪水。门开了，我惊喜地告诉自己，我还能见得着高社工。我的泪水似乎趋于感激。不，不可以。我从来没有在这见小屋里流过泪，一直是有着欢乐的身影，这一次我一定要忍住不能哭。我不敢看着高社工，一如往常坐在充满绿意的沙发上。我笑着对高社工说明天就要毕业了。可是我说着说着声音就变了，我再也没能压制住我复杂的内心，我失声恸哭。高社工坐到我的面前安慰我不要哭，我们于是便哭成一片…… 我记得你发给我邮件中的鼓励：“相信走过宝城小学，你的童年里一定有快乐的彩虹，在未来的路上，绽放夺目耀眼的光彩。” 追寻我上了初中，初一初二的时候经常回到小学。只要初中放学稍微早一点，我就会一路带着小跑，踏着夕晖，背着沉重的书包跑向宝城小学，跑向我的小学时光。 好几次，我都是只能见到一位老师或是甚至一个都见不到。等我飞奔到母校时，这时的天基本上已经黑了。我循着我上学的轨迹向着教学楼疾行，看着教学楼上泛起的点点灯光，我都会期望有我的恩师在等着我。有一次我见到了黄瑞金老师，有一次我见到了叶湘龙老师，还有一次见到了罗南南老师。每一次都是心怀希望地冲向社工室，但是每一次都会看到“学校社工室”紧锁着的木门。教学楼里的夜很凄凉，我穿着初中校服含泪倚傍在门板上，所有的月光都似乎被时光吞噬了，在热泪纵横的夜晚披在我身上的只有黑夜的幽邃。我在社工室门前苦苦哀嚎，可是没有人回应，只有附近厕所的流水声。我想尽一切办法，试图塞纸条找一切办法联系上。我只能对她留言，书写下我的思念，我也知道第二天她会看得到的，可是我就再也无法得到回复了，只能在心底遐思，想象该会得到怎样的鼓励。那天中午，我遇见了高社工。她的发型稍稍改变，不再是我脑海中的印象了。我心头有万千种话，可是我却结结巴巴地随便说了几句。你之前不是有很多话要对她说吗，你怎么这么不争气啊！只记得她说过“你长高了，长胖了，变声了，长大了……”我宁愿不长高，不长胖，不变声，不长大！我只愿时光永远停驻在美好的世界。那一天记得我们早早地在上午就放学了。我已经拥有自信，相信这次与时光的博弈我一定能战胜。于是就兴奋地冲向宝城，离开车流的喧嚣浮躁。我敲了敲社工室的门，里面有了走路的脚步声，我欣喜若狂。不料，刚想喊一声高社工却被一张陌生的面孔给阻挡了。 她是另一位女社工。我懵了一下，以为这辈子也遇不上高社工了，便沮丧地问她的消息。她随即呼了高社工的号码，我倾听，手机的那头传来的正是她的声音。那位女社工把手机给了我，心中正满是欣喜，等到我与眷念的故友通话的时候，便得知她转到了宝安中学初中部。我欣喜若狂，感激上天给我这么好的机会。 蜕变我来到了初中部高社工说的那个地方，就在宝安中学大圆形标志里的上方。那一次，我冒昧地闯了进去，看见里面不再是温馨的小屋了，取而代之的是工工整整摆放的茶几和书桌，充满绿意却被黑皮所取代的沙发，俨然是一间办公室。我想她应该是升了官吧，我心底虽然给予了她祝贺，但毕竟多少丧失原来的质朴了。高社工看到茫然的我，背着一副鼓鼓的书包，热情如故地接待了我。我按照习惯取下书包放在黑色的沙发上，问候她的近况，她以为我要写作业，便提醒了一句。我只不过觉得书包太重想卸重一下而已。第二次带着痴情的杨欣一起来的，遗憾的是WC和王肖童已经淡忘许久，我们向高社工问了次好便无声地离开了。之后就是近乎一年的隔绝，尽管身在咫尺。 这一年里，我的学业波澜起伏。对于那些不好的事情，是绝对不能跟此故友说的。于是我多想和她分享学业上的成就，多想和她聊聊事业上的心得，可是我怕打扰其事业，影响她的工作。那一天，我来到五楼，从远处看到她在与另几个人商量事情，身旁还有如山的书本，身上出现了领导者的风范。我等待良久，看见她在向我招手，我便兴奋地回应她，心中镶嵌着千万个太阳。远方一位认识的同学走来，我跟着他一起离开了，一路上难以抑制激动的心绪，我不断地乱对这同学感激地说谢谢，他自是莫名其妙，永远不会明白的了。 壹壹年的元旦前夕，我鼓起勇气走向她的办公室。办公室里有两个女的，一个是高社工，另外一个问她我是不是就是那个宝城的，我欣喜地说是，便向高社工祝愿新年快乐。六月份我们就要离去了，我们就要驰骋沙场，拼搏蓝天。我多么想收到这位故友对我的祝福啊，离别后不知道我们是否还能见面呢，不知道仅存的稚嫩是否还彼此存在呢？ “暴风雨过后，我们都长大了。”我依旧希望能有一颗永久童真的心灵，去细细回味朦胧壮阔曲折迷离的社工情。","link":"/2012/01/15/社工情/"},{"title":"第一次自由之行","text":"2015年9月19日，我大学生活的第一个周末，新生军训方才结束，正式课程还没有开始，一整天的时间空在那里，等待我去发掘。 因为对城市生活早已厌倦，顺着直觉，我一路向北，远离市区，往城市的边缘走，去看长江，去看海。滨江公园确实很美，湛蓝的天空下，一条路把我引入花与树的世界，五彩斑斓的风筝争相升起，地上铺满了交错纵横的树影，鲜花的芬芳顺风盘旋，小溪的粼粼波纹四散着耀眼的光芒。天空，太阳，鲜花，大树与我一见如故，我的心不自觉升起莫名的感动。 感动，我当然会感动，我当然应该意识到这是我人生的第一次，第一次进入自由。第一次真正离开家人去独自掌握自己的生活，第一次告别束缚顺着自己的心去探索，第一次没有烦恼、用一颗空灵的内心去感知这个我生活了17年的世界。我该是何其感激。我用手机放着我认为世界上最动听最扣人心弦的轻音乐，用最为舒缓的步伐走在柔软的草坪上，全身放松躺在波光粼粼的小河边，看头顶的大树筛下并不刺眼的阳光，静静地看着河边的芦苇随风摇摆。看着摄影爱好者举着相机对着自己的新作品流露满意的笑容，看着一家人哄着婴儿车里的孩子欣喜万分，看着公园草坪上的孩子一起奔跑身后留下串串七彩的泡泡。今天，我不必去担忧我还有作业没有写完，不必去在意我的成绩离我的目标还很遥远，不必去跟同学去比较分数的高低，不必去为家人的身体与和睦而烦恼，也尚不必去担心我的薪资我的生活我的未来。这时候只有我一个人，我一个人躺在公园的草坪，我一个人走在高大而神圣的树林中，用我的手机去记录这些催人泪下的美丽场景。 在美丽的树林里迷路，看到远处的木屋在召唤着我，我顺着一条笔直的长路走到了江边。我听见江边的人群在呼喊在拍照，闻到了沾有水汽的风的味道，看着蓝天下轮船缓缓移动，然后消失在我的视线中。远处的白色风车慢慢地旋转，另一头的灯塔静静屹立在水面中央，海浪扑打着岸边，海鸥划过天空，逍遥地一去不回，自在地不留痕迹。 自由的浪漫与无穷魅力，是我自记事以来不断追求的东西，而今是我真正意义上的第一次，说它是里程碑，一点都不过分。让我成为自由的使者，在我的青春里活得干脆潇洒，就像那些风筝，像那些柳枝，像那些波光，像那些海鸥。像风一样自由，我的青春也要一往无前，活出真正想见到的自己。","link":"/2015/09/19/第一次自由之行/"},{"title":"爸爸妈妈的年青时代","text":"我以前常常骄傲于找出我的年代与父母的不同之处，好像要向他们展示祖国新花朵生活环境的缤纷与灿烂。但是才慢慢发觉，爸爸妈妈的生活同样美丽，与父母拥有青春时期的共鸣点是何其幸福的事情。随着年龄的增长，两代人同样是在向青春致敬，因为青春这个东西，我与爸爸妈妈的心越来越近。回味着胶卷时代的相片，以及参阅种种时代的见证，我在爸爸妈妈的讲述中走进那个熟悉人物的别样时代。 爸爸妈妈的成长外公外婆在江西某核工业基地工作，于是妈妈一家人都在深山里面工作和生活。没有城市的喧嚣纷扰，他们却获得了一种逍遥自在的生活方式。坐在外公的汽车里，穿越原始的山林，几个激动的孩子左顾右盼，清新撩人的山风扑面，听水声潺潺看树影环抱。调皮的他们有时候偷吃当地老表的作物，有时候出来摘摘甘甜的野果，脚趟湍急的山溪，穿越长长的吊桥。妈妈和舅姨们说到这里总是心潮澎湃，对这种自然野性的生活念念不忘，常跟我说要带我回去好好体验一番，虽然当下已冷清荒凉。 妈妈有时候也会童性大发，把小小的头伸进铁门间，穿过来，穿过去，怡然自乐。不料，有一次当她把头伸进较窄的栏间时，头却被卡住了。她既害怕又着急，不知所措的她看到旁边菜地有个男的捧腹大笑她的窘样，尴尬之余更多生气。几经挣扎，忽地一下就把头抽了出来，对他做了个鬼脸，跑走了。 妈妈说她最怕的就是山里的蛇。小时候，妈妈会独自山上捡柴火，有一次天快黑了，于是就背着柴火加快脚步往回跑。跑着跑着，树上的一条蛇倏然掉到她的脖子上，妈妈瞬间感觉脖子一阵冰凉，那股凉气瞬间在她体内扩散开来。她吓得没有动弹，屏住呼吸，眼睁睁地看着蛇在她身上冷峻地爬来，再从她的身上爬走。 孩子们很懂事，家里很少能吃上鸡蛋，外婆用家里难得吃一次的酒酿蛋招待客人的时候，小小的他们在一旁直流口水但却没有吵着要吃；在弟弟受人欺负的时候，姐姐们义愤填膺合伙找他算账；在外婆生病的时候，妈妈与大姨上山砍柴照顾小姨和舅舅……学业根本不是他们所担心顾虑的事情，在醇美的大山里养成了天真纯朴的美丽性格。 爸爸就没那么幸运能待在国营企业，爷爷奶奶是标准的湖南农民。从小家里很苦，奶奶的身体又不好。爸爸每天上学都要早早起来自己做饭，越过几个山头到赤马镇里读书，回到家又得牵着牛跑。于是在我和爸爸都同时偷懒的时候，爸爸总爱对我说：“像你这么大的时候我每天放完学就去放牛。”他们几个兄弟都陆续地上学，人人都盼望着有一日能改变单调的生活，盼望着能过上幸福的日子。爸爸在长沙市里度过了它的大学时光，常常在城市与农村之间奔走，有时会住在对他关怀备至的亲戚家里，一点一滴的感动坚定他追求理想的信念。在长沙的时候，没有亲人，只好独自享受属于自己的孤单与寂寞。他还尝试过在路边卖衣服，凝望着人群熙攘，探寻着将来未知而莫测的路。我能想象出小小的他在为生计奔波的样子。爸爸的求学路确实很艰辛，不仅是在物质上的匮乏，更是精神上的历练。 南下，南下1993年，爸爸妈妈顺着改革开放的洪流一路南下，聚在了神秘而陌生的深圳。那时候的深圳，既不是邓主席刚刚画过圈的宁谧寂静的小渔村，也不是现在的繁华国际大都市，而更是像在浇淋暴风雨中的笋林。如果说深圳是浴火的凤凰，那么那时正是人们燃烧自己的时候。 爸爸与另一个同学扛着塑料条编织的经典行囊，带着自己积攒的有限的现金从火车站出来，来到改革开放的风口浪尖，随即开始了迷茫寻找工作的历程。一天天过去，在人才市场的闯荡与尝试考验着他们的耐力与精神。内心的无助在几天下来腰包已经干瘪、工作没有着落的情况下变得无限放大。就在他们最最绝望的时候，爸爸被南山一家台商办的公司招为电子工程师，飘摇的小船终于得到平静。 妈妈的来深道路则简单很多，国营单位将老员工的待就业子女被车拉到了深圳，分配到了同样的那家公司。初来乍到，从干粗活到流水线，在国营单位父母陪伴、大山里新鲜空气下长大的孩子第一次离开父母。她们辛勤工作，盼着有一天能够光荣地拿着挣来的钱回到家里见父母，告诉他们：你们的孩子长大了，在外头打拼挣钱了。当每年年底快过节的时候，单位派车来接她们回去。车声传入她们的耳畔，她们老远看到车来了，就纷纷冲下楼，泪眼汪汪地准备回家；家里的人早已做好饭，在马路上迎接着汽车的归来，游子们含泪下车之时总会受到家里人温情的拥抱。 为爱痴狂妈妈在不断努力工作后被安排到流水线当主管，爸爸也在品管部当经理。女方美丽的容颜以及严谨细致的工作态度深深将男方吸引，血液中迸发出追求爱情的灵魂，他们的爱情故事由此展开。说到这里，身为孩子的我总是感到特别别扭，如果我穿越到那个时间，看到年轻的爸爸妈妈坐在一起，以接近他们年龄的面孔告诉他们我是你们的未来的孩子，他们会有什么想法呢？ 妈妈说很多男生都追求过她，但是傻傻的她仍然选择了傻傻的爸爸。为了得到这份真挚的爱情，爸爸有时候会做出现在我无法想象的事情。比如在别的男同志欺负她的时候，他会勇敢地站出来，拿刀相向。 听到这里我才知道原来爸爸妈妈的爱情有这么坚固诺言和不可阻遏的火花，对于孩子来说，父母之间有如此坚毅赤诚的感情是如此惬意！ 爱情的力量使得他们最终走在一起，妈妈带着一起下海的大姨跟着爸爸回到了湖南爷爷家。因为爸爸是爷爷家第一个南下打拼的孩子，妈妈也就成为爷爷家附近第一个远方来的媳妇。大家看着这位从外地来的媳妇，红色的婚礼红色的衣，红色的嘴唇红色的情，他们成为婚礼宴席的主角，在土房子里许下终身的诺言。 回到深圳，不久，爸爸妈妈就从宿舍搬走，在公司附近租了间小房子。在人人都为生计奋斗的时代，他们共同吃一碗饭，两个人面对面额对额吃着不多的米饭。亲密无间的爱情在属于他们的时代背景下就这么简单浪漫地荡漾，像宁静的夜晚广阔的大海上温暖的亮着渔火的船，视角再远再远仍然能看得清楚那盏橘黄的灯，时空再遥远仍然能感受到那么明亮柔情的光芒。 他们一点一点积攒着钱，把自己赚的血汗钱分成几份，一份给爷爷家、一份给外公家、一份留给他们、一份留给未来的我。 痛苦中诞生希望爸爸妈妈95年结婚后，攒了几万块才真正有充足的经济能够在98年生了我。他们在深圳认识，但为了让我有根的感觉，就一起回到了爸爸的老家浏阳。妈妈尽管对浏阳的很多东西不适应，语言和饮食都不通，再加上身处异地思念亲人，但最终还是克服了这些挫折。原本不吃辣椒的妈妈也渐渐地接受了它，现在倒成为一个辣椒达人。 怀孕的那段时间，妈妈从一个在大山里玩惯的孩子走向苦闷枯乏顶着大肚子的母亲。半年多不能看电视，害怕有辐射会影响胎儿，只能听着收音机里无言的音乐；寒冬时期，当外头冰霜覆盖的时候，妈妈冷到发抖却不能烤火，只能窝在被子里忍受寒冷与寂寞。 听妈妈说，在我出生的那一天，湖南下着扑面的大雪。两个伯伯用简易的轿子抬着疼痛不已的妈妈从遥远的爷爷家抬到镇上，我在那里的卫生院被接生了出来。 可是不那么幸运，妈妈出现了严重的产后不良症状。 妈妈在生我之后无法正常排尿，只能通过插管子来定时释放。镇里的医生一见到这种情况，二话不说把妈妈的点滴拔掉，要求我们转院。爸爸在镇里临时租了个面包车，心急如焚地将妈妈送到了另一个镇上，那里的医生勇敢地处理了几天，没想到病情加剧。没有办法，我们只能坐车到浏阳市去看病。妈妈被送到了浏阳市的一家医院。 我看着转院浏阳的病历观察本，几页的本子被医生的钢笔字迹填满。每一字每一句都深刻地描述着妈妈当时躺在病床上痛苦的经历。我从来没有看过哪个医生把病历本写得这么认真、这么掷地有声，在妈妈刚生我之后的病历本这里，我算是开了眼界。妈妈痛苦的时候，曾经绝望地向爸爸说：“我从房子上跳下去好不好。” 很感激医生把妈妈的病状一五一十地描绘出来，让我能够感觉得到好像现在的我就在那里，就在妈妈身旁陪着她，陪着她一起反抗病魔，在她身边一边内疚于我的诞生。 我看着病历本上记录的日期逐渐推移，看到妈妈的症状正不断好转，看到医生笔下满满病历上最后两个字写着出院，我的心如沉石一般落下。这是我看过的最惊心动魄的故事情节。 从那时起，我便正式地被这个世界邀请进来。作为爸爸妈妈对未来的希望，小小的我身当重任却毫无察觉，新奇地观察着这个世界的每一个角落，感觉这个世界就是我的游乐场。 不平静的社会在被子里裹着，装在一个箱子里，插在拥挤的人群中，放在轰隆轰隆的火车上，人们看着我说：“这么小的小孩就跟着南下了哈。” 坐了许多年的火车，以至于我现在听到火车跨过无数铁轨的声音都感到无比亲切。 当时的深圳还不能说是改革开放带来新世界，仅凭比内地多一些高楼大厦是说明不了什么的。在爸爸妈妈口中，那是一个令人胆战心惊的时代。每天深夜都会有警车的蜂鸣声无止息地刺耳，派出所日夜不息地巡查所谓的三无人员，在马路上必须要带上证明自己的身份证件，如果没有就会被抓到派出所拘留。那时候红树林是被警察重点监控的，人迹罕至，也就多鱼蟹。妈妈有一位同事因好玩而去红树林抓螃蟹，没有带身份证件而被警察抓走，听说她在拘留地忍受了无法忍耐的酷刑，睡在厕所里，不停地搞卫生，有人来的时候还饱受凌辱。几个星期后被公司花钱保释出来，她后悔地说：“我这辈子再也不冒险了。” 不仅是警察的乱，还有恶势力的猖獗。妈妈有一对同事情侣，他们深夜在住宅区附近的小河旁约会。黑幕中，几个高大的歹徒拿刀直指双方，歹徒对男方恐吓道：“要钱还是要女友？”，男的吓得瑟瑟发抖，把身上所有的钱都交了出来，不料，歹徒接过钱之后没有放走女友，反而将她拐走。几个月后，那个女的回到公司，衣服破败不堪，什么话也没说，收拾完行李就直接走了。 老舅来深圳的时候，在市内找了很多工作，也被骗了不少。有一次，他来到地王大厦旁边的一幢金色的大厦里，准备投简历。前台的人让他交四百块钱的介绍费，他只交了身上仅有的两百。他在一旁等待的时候，旁边的一位老伯告诫他：“你还是不要等了，快走吧。” 这时候老舅才感到一阵不妙，想向前台要回他的两百块钱。这时候他发现前台的墙壁是可以活动的，而且有人在里面，于是就赶紧走，也放弃了要回两百块钱的念头。还有一次，在另一个地方投简历的时候，直接被公司的车拉走，拉到关外拔草，干了一个上午才被放走。 对比十多年后的深圳，这些无理由的人权的侵犯简直是不可理喻。我曾经还觉得在八九十年代生活比现在轻松自由得多，没想到每个年代都有每个年代的血与泪。 我的梦幻童年我在家人的呵护下渐渐长大，每每回想起小时候经历的事情总是那么如梦似幻。很小的时候还住在南山，家人常常带着我去华侨城那一带玩耍，对于我来说，华侨城简直是美丽到了极点的地方。在华侨城呆过的每一帧记忆都是那么令人心驰神往。 首先要提到的就是世界之窗和锦绣中华那一段，作为深圳的标志之一，那一块的风景在我看来特别的高贵而圣洁。每天晚上，世界之窗广场的喷泉下总会亮着绿色的亮光，光线把一整串的水柱照得透亮，喷泉的水像会流动的晶莹的绿宝石。我常常跑上喷泉旁去看美丽的灯光，绿色的灯光隔着水幕照样亮得刺眼，使我既不情愿又非常舒服地闭上眼睛。在白天的时候，广场上还有许许多多的胖鸽子，我一见到它们成堆地在一起吃东西，便会开心地冲向它们，看它们迟钝的样子吃还没吃饱就被我赶飞的窘态。在阳光下跟着鸽子一起奔跑。 以前的被子用久了就会有棉粒。小的时候，我一直傻傻地坚信，只要把被子上的棉粒投进世界之窗广场的喷泉里面，它就会产生爆炸一样的水花。好不容易有几次机会来世界之窗，可惜总是忘记从家里的被子上取下棉粒。我想，这应该是我小时候最大的心结了吧。我等了好久好久，那一次我又跟家人来到了世界之窗，依旧忘带被子上的棉粒。无奈，敲破脑袋想办法，赶紧在衣服裤子上拔下几粒出来，用尽全力向喷泉里扔，可是棉粒轻轻的却总是被喷泉的水汽弹回。 不得不提的是华侨城那一段的景色。好像进入梦境一般，被繁密的树荫包裹住的人行道上，依稀能见到阳光的剪影，偶尔能听见鸟雀的吟叫，妈妈牵着我的手，顺着青葱寂静的人行道，走向天空中的游乐场。一点也不夸张，那里的景色依旧是这么令人陶醉，在我脑海里回荡十余年不改本色。所谓“天空中”，其实是游乐场建在一幢房顶天台上。那幢大楼有一扇很大的玻璃门，走进去，可以看到盘旋上升的旋转楼梯。下面几层好像是一户保龄球馆，但是我从来没有进去过，我关注的是顶层阳光下的游乐场。 不知道谁有这么圣洁的创意，把游乐场设在房子的最顶层。从那个游乐场向下俯瞰，能看到整个世界之窗以及深南大道的美景。我想，这应该是大人们前所未有的对儿童的尊敬。我在游乐设施间陶醉的时候，妈妈便坐在远处的木椅上看着我笑。 搬迁关外因为爸爸的公司迁出特区，我们家也跟着一起来到了宝安。我们住在西乡密集街巷里的一个庭院中，开了一个小厂，我也住在里面。我很怀念住在南山的日子，每年都在数着我生命中留在南山的时间与留在宝安的时间，等到我呆在宝安的时间超过了南山的时候，我才慢慢意识到我成为了一个宝安人。 那时候妈妈每天工作下班便走几公里从幼儿园接我回家。要回到我们家，得先穿过嘈杂喧闹的街市，经过栉次邻比的水泥房。妈妈牵着我的手，在路上时不时能看见改建的楼房和日夜不息的水泥搅拌车，妈妈不厌其烦地跟我说少闻些水泥味，天真的我却觉得挺香的，一边笑一边大口大口做着吸气状惹妈妈开心，一笑就笑到日暮黄昏。 我深深记得，妈妈刚强地抱着我穿过了附近新铺水泥的马路；记得她用大大的手护住我的头，带我穿过建筑护网包围的危险地段；记得我们为了不闻臭味而摒息向着前方奋力地向前冲刺；记得在我眼睛掉进沙子后妈妈一边吹着我的眼睛一边念着神奇的咒语：“小石头，吃沙子，吃得进，吃掉去，吃不进，吐出来，咕噜咕噜噜……” 有一次宝安内涝，家里的庭院积了挺深的积水，积水深达一个成年人膝盖的高度。但是我很激动。我依稀记得爸爸告诫我不要自己下水走路，小心地面上看不见的松动的井盖。爸爸背着我穿过被雨浸过的灰色的水巷，肩膀不大但很硬朗，身材消瘦但不失气力，身上的味道有他的特征。我在他的背后可以无所畏惧恣意玩耍。怕什么料峭风雨，爸爸的肩膀也轻胜马。 渐渐地我上了小学，每天放学都能看到妈妈的身影。手里拿个牛肉串，我不断让妈妈讲故事，或者不停地问她为什么。跟她一起分享小学学到的一些小知识，一起学习入门英语，向妈妈炫耀两位数乘十一的快速运算，又或者是讲述一天中发生的事情。妈妈与我无话不说，没有秘密，我们像对方的一面镜子，在微笑的时候不忧愁、悲伤的时候不喜悦。 之后，我们家在新安买了房子，住在一个将来近乎十年呆的处所里。当我逐渐发现妈妈讲的故事开始重复，或者编得不符合逻辑的时候，我便开始了创造我自己年青时代的努力。","link":"/2014/07/28/爸爸妈妈的年青时代/"},{"title":"Pig的一些基础概念及用法总结[转]","text":"本文转载自Apache Pig的一些基础概念及用法总结 本文可以让刚接触pig的人对一些基础概念有个初步的了解。本文大概是互联网上第一篇公开发表的且涵盖大量实际例子的Apache Pig中文教程（由Google搜索可知），文中的大量实例都是作者Darran Zhang（website: codelast.com）在工作、学习中总结的经验或解决的问题，并且添加了较为详尽的说明及注解，此外，作者还在不断地添加本文的内容，希望能帮助一部分人。 Apache pig是用来处理大规模数据的高级查询语言，配合Hadoop使用，可以在处理海量数据时达到事半功倍的效果，比使用Java，C++等语言编写大规模数据处理程序的难度要小N倍，实现同样的效果的代码量也小N倍。Twitter就大量使用pig来处理海量数据&mdash;&mdash;有兴趣的，可以看Twitter工程师写的这个PPT。但是，刚接触pig时，可能会觉得里面的某些概念以及程序实现方法与想像中的很不一样，甚至有些莫名，所以，你需要仔细地研究一下基础概念，这样在写pig程序的时候，才不会觉得非常别扭。本文基于以下环境：pig 0.8.1 先给出两个链接：pig参考手册1，pig参考手册2。本文的部分内容来自这两个手册，但涉及到翻译的部分，也是我自己翻译的，因此可能理解与英文有偏差，如果你觉得有疑义，可参考英文内容。 【配置Pig语法高亮】在正式开始学习Pig之前，你首先要明白，配置好编辑器的Pig语法高亮是很有用的，它可以极大地提高你的工作效率。 如果你在Windows下编写Pig代码，好像还真没有什么轻量级的编辑器插件（例如Notepad++的插件之类的）可以实现对.pig文件的语法高亮显示，我建议你使用Notepad++，在&ldquo;User Define Language&rdquo;中自定义Pig语法高亮方案（我这样做之后感觉效果很好）；如果你觉得麻烦，那么你可以直接用Notepad++以SQL的语法高亮来查看Pig代码，这样的话可以高亮Pig中的一部分关键字。在Linux下，选择就很多了，大分部人使用的是vi，vim，但我是个Emacs控，所以我就先说说如何配置Emacs的Pig语法高亮。此插件是一个很好的选择：https://github.com/cloudera/piglatin-mode 那么，怎么使用这个插件呢？ 下载piglatin.el文件，将它放置在任何地方&mdash;&mdash;当然，为了方便，最好是放在你登录用户的根目录下（也就是与.emacs配置文件在同一目录下），然后将其重命名为 &ldquo;.piglatin.el&rdquo;注意前面是有一个点的，也就是说将这个文件设置成隐藏文件，否则可能会误删了。然后，在 .emacs 文件中的最后，添加上如下一行： (load-file &quot;/home/abc/.piglatin.el&quot;) 这里假设了你的 .piglatin.el 文件放置的位置是在 /home/abc/ 目录下，也就是说emacs会加载这个文件，实现语法高亮显示。 现在，你再打开一个.pig文件试试看？非常令人赏心悦目的高亮效果就出来了。效果如下图所示： 其实Emacs也有Windows版的，如果你习惯在Windows下工作，完全可以在Windows下按上面的方法配置一下Pig语法高亮（但是Windows版的Emacs还需要一些额外的配置工作，例如修改注册表等，所以会比在Linux下使用要麻烦一些，具体请看这篇文章）。文章来源：http://www.codelast.com/下面开始学习Pig。 （1）关系（relation）、包（bag）、元组（tuple）、字段（field）、数据（data）的关系 一个关系（relation）是一个包（bag），更具体地说，是一个外部的包（outer bag）。 一个包（bag）是一个元组（tuple）的集合。在pig中表示数据时，用大括号{}括起来的东西表示一个包&mdash;&mdash;无论是在教程中的实例演示，还是在pig交互模式下的输出，都遵循这样的约定，请牢记这一点，因为不理解的话就会对数据结构的掌握产生偏差。 一个元组（tuple）是若干字段（field）的一个有序集（ordered set）。在pig中表示数据时，用小括号()括起来的东西表示一个元组。 一个字段是一块数据（data）。 &ldquo;元组&rdquo;这个词很抽象，你可以把它想像成关系型数据库表中的一行，它含有一个或多个字段，其中，每一个字段可以是任何数据类型，并且可以有或者没有数据。&ldquo;关系&rdquo;可以比喻成关系型数据库的一张表，而上面说了，&ldquo;元组&rdquo;可以比喻成数据表中的一行，那么这里有人要问了，在关系型数据库中，同一张表中的每一行都有固定的字段数，pig中的&ldquo;关系&rdquo;与&ldquo;元组&rdquo;之间，是否也是这样的情况呢？不是的。&ldquo;关系&rdquo;并不要求每一个&ldquo;元组&rdquo;都含有相同数量的字段，并且也不会要求各&ldquo;元组&rdquo;中在相同位置处的字段具有相同的数据类型（太随意了，是吧？）文章来源：http://www.codelast.com/（2）一个 计算多维度组合下的平均值 的实际例子为了帮助大家理解pig的一个基本的数据处理流程，我造了一些简单的数据来举个例子&mdash;&mdash;假设有数据文件：a.txt（各数值之间是以tab分隔的）： [root@localhost pig]$ cat a.txta 1 2 3 4.2 9.8a 3 0 5 3.5 2.1b 7 9 9 - -a 7 9 9 2.6 6.2a 1 2 5 7.7 5.9a 1 2 3 1.4 0.2 问题如下：怎样求出在第2、3、4列的所有组合的情况下，最后两列的平均值分别是多少？例如，第2、3、4列有一个组合为（1，2，3），即第一行和最后一行数据。对这个维度组合来说，最后两列的平均值分别为：（4.2+1.4）/2＝2.8（9.8+0.2）/2＝5.0而对于第2、3、4列的其他所有维度组合，都分别只有一行数据，因此最后两列的平均值其实就是它们自身。特别地，组合（7，9，9）有两行记录：第三、四行，但是第三行数据的最后两列没有值，因此它不应该被用于平均值的计算，也就是说，在计算平均值时，第三行是无效数据。所以（7，9，9）组合的最后两列的平均值为 2.6 和 6.2。我们现在用pig来算一下，并且输出最终的结果。先进入本地调试模式（pig -x local），再依次输入如下pig代码： A = LOAD 'a.txt' AS (col1:chararray, col2:int, col3:int, col4:int, col5:double, col6:double);B = GROUP A BY (col2, col3, col4);C = FOREACH B GENERATE group, AVG(A.col5), AVG(A.col6);DUMP C; pig输出结果如下： ((1,2,3),2.8,5.0)((1,2,5),7.7,5.9)((3,0,5),3.5,2.1)((7,9,9),2.6,6.2) 这个结果对吗？手工算一下就知道是对的。文章来源：http://www.codelast.com/下面，我们依次来看看每一句pig代码分别得到了什么样的数据。①加载 a.txt 文件，并指定每一列的数据类型分别为 chararray（字符串），int，int，int，double，double。同时，我们还给予了每一列别名，分别为 col1，col2，&hellip;&hellip;，col6。这个别名在后面的数据处理中会用到&mdash;&mdash;如果你不指定别名，那么在后面的处理中，就只能使用索引（$0，$1，&hellip;&hellip;）来标识相应的列了，这样可读性会变差，因此，在列固定的情况下，还是指定别名的好。将数据加载之后，保存到变量A中，A的数据结构如下： A: {col1: chararray,col2: int,col3: int,col4: int,col5: double,col6: double} 可见，A是用大括号括起来的东西。根据本文前面的说法，A是一个包（bag）。这个时候，A与你想像中的样子应该是一致的，也就是与前面打印出来的 a.txt 文件的内容是一样的，还是一行一行的类似于&ldquo;二维表&rdquo;的数据。文章来源：http://www.codelast.com/②按照A的第2、3、4列，对A进行分组。pig会找出所有第2、3、4列的组合，并按照升序进行排列，然后将它们与对应的包A整合起来，得到如下的数据结构： B: {group: (col2: int,col3: int,col4: int),A: {col1: chararray,col2: int,col3: int,col4: int,col5: double,col6: double}} 可见，A的第2、3、4列的组合被pig赋予了一个别名：group，这很形象。同时我们也观察到，B的每一行其实就是由一个group和若干个A组成的&mdash;&mdash;注意，是若干个A。这里之所以只显示了一个A，是因为这里表示的是数据结构，而不表示具体数据有多少组。实际的数据为： ((1,2,3),{(a,1,2,3,4.2,9.8),(a,1,2,3,1.4,0.2)})((1,2,5),{(a,1,2,5,7.7,5.9)})((3,0,5),{(a,3,0,5,3.5,2.1)})((7,9,9),{(b,7,9,9,,),(a,7,9,9,2.6,6.2)}) 可见，与前面所说的一样，组合（1，2，3）对应了两行数据，组合（7，9，9）也对应了两行数据。这个时候，B的结构就不那么明朗了，可能与你想像中有一点不一样了。文章来源：http://www.codelast.com/③计算每一种组合下的最后两列的平均值。根据上面得到的B的数据，你可以把B想像成一行一行的数据（只不过这些行不是对称的），FOREACH 的作用是对 B 的每一行数据进行遍历，然后进行计算。GENERATE 可以理解为要生成什么样的数据，这里的 group 就是上一步操作中B的第一项数据（即pig为A的第2、3、4列的组合赋予的别名），所以它告诉了我们：在数据集 C 的每一行里，第一项就是B中的group&mdash;&mdash;类似于（1，2，5）这样的东西）。而 AVG(A.col5) 这样的计算，则是调用了pig的一个求平均值的函数 AVG，用于对 A 的名为 col5 的列求平均值。前文说了，在加载数据到A的时候，我们已经给每一列起了个别名，col5就是倒数第二列。到这里，可能有人要迷糊了：难道 AVG(A.col5) 不是表示对 A 的col5这一列求平均值吗？也就是说，在遍历B（FOREACH B）的每一行时候，计算结果都是相同的啊！事实上并不是这样。我们遍历的是B，我们需要注意到，B的数据结构中，每一行数据里，一个group对应的是若干个A，因此，这里的 A.col5，指的是B的每一行中的A，而不是包含全部数据的那个A。拿B的第一行来举例：((1,2,3),{(a,1,2,3,4.2,9.8),(a,1,2,3,1.4,0.2)})遍历到B的这一行时，要计算AVG(A.col5)，pig会找到&nbsp;(a,1,2,3,4.2,9.8) 中的4.2，以及(a,1,2,3,1.4,0.2)中的1.4，加起来除以2，就得到了平均值。同理，我们也知道了AVG(A.col6)是怎么算出来的。但还有一点要注意的：对(7,9,9)这个组，它对应的数据(b,7,9,9,,)里最后两列是无值的，这是因为我们的数据文件对应位置上不是有效数字，而是两个&ldquo;-&rdquo;，pig在加载数据的时候自动将它置为空了，并且计算平均值的时候，也不会把这一组数据考虑在内（相当于忽略这组数据的存在）。到了这里，我们不难理解，为什么C的数据结构是这样的了： C: {group: (col2: int,col3: int,col4: int),double,double} 文章来源：http://www.codelast.com/④DUMP C就是将C中的数据输出到控制台。如果要输出到文件，需要使用： STORE C INTO 'output'; 这样pig就会在当前目录下新建一个&ldquo;output&rdquo;目录（该目录必须事先不存在），并把结果文件放到该目录下。 请想像一下，如果要实现相同的功能，用Java或C++写一个Map-Reduce应用程序需要多少时间？可能仅仅是写一个build.xml或者Makefile，所需的时间就是写这段pig代码的几十倍了！正因为pig有如此优势，它才得到了广泛应用。文章来源：http://www.codelast.com/（3）怎样统计数据行数在SQL语句中，要统计表中数据的行数，很简单： SELECT COUNT(*) FROM table_name WHERE condition 在pig中，也有一个COUNT函数，在pig手册中，对COUNT函数有这样的说明： Computes the number of elements in a bag. 假设要计算数据文件a.txt的行数： [root@localhost pig]$ cat a.txta 1 2 3 4.2 9.8a 3 0 5 3.5 2.1b 7 9 9 - -a 7 9 9 2.6 6.2a 1 2 5 7.7 5.9a 1 2 3 1.4 0.2 你是否可以这样做呢： A = LOAD 'a.txt' AS (col1:chararray, col2:int, col3:int, col4:int, col5:double, col6:double);B = COUNT(*);DUMP B; 答案是：绝对不行。pig会报错。pig手册中写得很明白： Note: You cannot use the tuple designator () with COUNT; that is, COUNT() will not work. 那么，这样对某一列计数行不行呢： B = COUNT(A.col2); 答案是：仍然不行。pig会报错。这就与我们想像中的&ldquo;正确做法&rdquo;有点不一样了：我为什么不能直接统计一个字段的数目有多少呢？刚接触pig的时候，一定非常疑惑这样明显&ldquo;不应该出错&rdquo;的写法为什么行不通。要统计A中含col2字段的数据有多少行，正确的做法是： A = LOAD 'a.txt' AS (col1:chararray, col2:int, col3:int, col4:int, col5:double, col6:double);B = GROUP A ALL;C = FOREACH B GENERATE COUNT(A.col2);DUMP C; 输出结果： (6) 表明有6行数据。如此麻烦？没错。这是由pig的数据结构决定的。文章来源：http://www.codelast.com/在这个例子中，统计COUNT(A.col2)和COUNT(A)的结果是一样的，但是，如果col2这一列中含有空值： [root@localhost pig]$ cat a.txta 1 2 3 4.2 9.8a 0 5 3.5 2.1b 7 9 9 - -a 7 9 9 2.6 6.2a 1 2 5 7.7 5.9a 1 2 3 1.4 0.2 则以下pig程序及执行结果为： grunt&gt; A = LOAD 'a.txt' AS (col1:chararray, col2:int, col3:int, col4:int, col5:double, col6:double);grunt&gt; B = GROUP A ALL;grunt&gt; C = FOREACH B GENERATE COUNT(A.col2);grunt&gt; DUMP C;(5) 可见，结果为5行。那是因为你LOAD数据的时候指定了col2的数据类型为int，而a.txt的第二行数据是空的，因此数据加载到A以后，有一个字段就是空的： grunt&gt; DUMP A;(a,1,2,3,4.2,9.8)(a,,0,5,3.5,2.1)(b,7,9,9,,)(a,7,9,9,2.6,6.2)(a,1,2,5,7.7,5.9)(a,1,2,3,1.4,0.2) 在COUNT的时候，null的字段不会被计入在内，所以结果是5。 The COUNT function follows syntax semantics and ignores nulls. What this means is that a tuple in the bag will not be counted if the first field in this tuple is NULL. If you want to include NULL values in the count computation, use COUNT_STAR. 文章来源：http://www.codelast.com/ （4）FLATTEN操作符的作用 这个玩意一开始还是挺让我费解的。从字面上看，flatten就是&ldquo;弄平&rdquo;的意思，但是在对一个pig的数据结构操作时，flatten到底是&ldquo;弄平&rdquo;了什么，又有什么作用呢？我们还是采用前面的a.txt数据文件来说明： [root@localhost pig]$ cat a.txta 1 2 3 4.2 9.8a 3 0 5 3.5 2.1b 7 9 9 - -a 7 9 9 2.6 6.2a 1 2 5 7.7 5.9a 1 2 3 1.4 0.2 如果我们按照前文的做法，计算多维度组合下的最后两列的平均值，则： grunt&gt; A = LOAD 'a.txt' AS (col1:chararray, col2:int, col3:int, col4:int, col5:double, col6:double);grunt&gt; B = GROUP A BY (col2, col3, col4);grunt&gt; C = FOREACH B GENERATE group, AVG(A.col5), AVG(A.col6);grunt&gt; DUMP C;((1,2,3),2.8,5.0)((1,2,5),7.7,5.9)((3,0,5),3.5,2.1)((7,9,9),2.6,6.2) 可见，输出结果中，每一行的第一项是一个tuple（元组），我们来试试看 FLATTEN 的作用： grunt&gt; A = LOAD 'a.txt' AS (col1:chararray, col2:int, col3:int, col4:int, col5:double, col6:double);grunt&gt; B = GROUP A BY (col2, col3, col4);grunt&gt; C = FOREACH B GENERATE FLATTEN(group), AVG(A.col5), AVG(A.col6);grunt&gt; DUMP C;(1,2,3,2.8,5.0)(1,2,5,7.7,5.9)(3,0,5,3.5,2.1)(7,9,9,2.6,6.2) 看到了吗？被 FLATTEN 的group本来是一个元组，现在变成了扁平的结构了。按照pig文档的说法，FLATTEN用于对元组（tuple）和包（bag）&ldquo;解嵌套&rdquo;（un-nest）： The FLATTEN operator looks like a UDF syntactically, but it is actually an operator that changes the structure of tuples and bags in a way that a UDF cannot. Flatten un-nests tuples as well as bags. The idea is the same, but the operation and result is different for each type of structure. &nbsp; For tuples, flatten substitutes the fields of a tuple in place of the tuple. For example, consider a relation that has a tuple of the form (a, (b, c)). The expression GENERATE $0, flatten($1), will cause that tuple to become (a, b, c). 文章来源：http://www.codelast.com/所以我们就看到了上面的结果。在有的时候，不&ldquo;解嵌套&rdquo;的数据结构是不利于观察的，输出这样的数据可能不利于外围数程序的处理（例如，pig将数据输出到磁盘后，我们还需要用其他程序做后续处理，而对一个元组，输出的内容里是含括号的，这就在处理流程上又要多一道去括号的工序），因此，FLATTEN提供了一个让我们在某些情况下可以清楚、方便地分析数据的机会。 （5）关于GROUP操作符在上文的例子中，已经演示了GROUP操作符会生成什么样的数据。在这里，需要说得更理论一些： 用于GROUP的key如果多于一个字段（正如本文前面的例子），则GROUP之后的数据的key是一个元组（tuple），否则它就是与用于GROUP的key相同类型的东西。 GROUP的结果是一个关系（relation），在这个关系中，每一组包含一个元组（tuple），这个元组包含两个字段：（1）第一个字段被命名为&ldquo;group&rdquo;&mdash;&mdash;这一点非常容易与GROUP关键字相混淆，但请区分开来。该字段的类型与用于GROUP的key类型相同。（2）第二个字段是一个包（bag），它的类型与被GROUP的关系的类型相同。 （6）把数据当作&ldquo;元组&rdquo;（tuple）来加载还是假设有如下数据： [root@localhost pig]$ cat a.txta 1 2 3 4.2 9.8a 3 0 5 3.5 2.1b 7 9 9 - -a 7 9 9 2.6 6.2a 1 2 5 7.7 5.9a 1 2 3 1.4 0.2 如果我们按照以下方式来加载数据： A = LOAD 'a.txt' AS (col1:chararray, col2:int, col3:int, col4:int, col5:double, col6:double); 那么得到的A的数据结构为： grunt&gt; DESCRIBE A;A: {col1: chararray,col2: int,col3: int,col4: int,col5: double,col6: double} 如果你要把A当作一个元组（tuple）来加载： A = LOAD 'a.txt' AS (T : tuple (col1:chararray, col2:int, col3:int, col4:int, col5:double, col6:double)); 也就是想要得到这样的数据结构： grunt&gt; DESCRIBE A;A: {T: (col1: chararray,col2: int,col3: int,col4: int,col5: double,col6: double)} 那么，上面的方法将得到一个空的A： grunt&gt; DUMP A;()()()()()() 那是因为数据文件a.txt的结构不适合于这样加载成元组（tuple）。文章来源：http://www.codelast.com/如果有数据文件b.txt： [root@localhost pig]$ cat b.txt(a,1,2,3,4.2,9.8)(a,3,0,5,3.5,2.1)(b,7,9,9,-,-)(a,7,9,9,2.6,6.2)(a,1,2,5,7.7,5.9)(a,1,2,3,1.4,0.2) 则使用上面所说的加载方法及结果为： grunt&gt; A = LOAD 'b.txt' AS (T : tuple (col1:chararray, col2:int, col3:int, col4:int, col5:double, col6:double));grunt&gt; DUMP A;((a,1,2,3,4.2,9.8))((a,3,0,5,3.5,2.1))((b,7,9,9,,))((a,7,9,9,2.6,6.2))((a,1,2,5,7.7,5.9))((a,1,2,3,1.4,0.2)) 可见，加载的数据的结构确实被定义成了元组（tuple）。 （7）在多维度组合下，如何计算某个维度组合里的不重复记录的条数以数据文件 c.txt 为例： [root@localhost pig]$ cat c.txta 1 2 3 4.2 9.8 100a 3 0 5 3.5 2.1 200b 7 9 9 - - 300a 7 9 9 2.6 6.2 300a 1 2 5 7.7 5.9 200a 1 2 3 1.4 0.2 500 问题：如何计算在第2、3、4列的所有维度组合下，最后一列不重复的记录分别有多少条？例如，第2、3、4列有一个维度组合是（1，2，3），在这个维度维度下，最后一列有两种值：100 和 500，因此不重复的记录数为2。同理可求得其他的记录条数。pig代码及输出结果如下： grunt&gt; A = LOAD 'c.txt' AS (col1:chararray, col2:int, col3:int, col4:int, col5:double, col6:double, col7:int);grunt&gt; B = GROUP A BY (col2, col3, col4);grunt&gt; C = FOREACH B {D = DISTINCT A.col7; GENERATE group, COUNT(D);};grunt&gt; DUMP C;((1,2,3),2)((1,2,5),1)((3,0,5),1)((7,9,9),1) 我们来看看每一步分别生成了什么样的数据：①LOAD不用说了，就是加载数据；②GROUP也不用说了，和前文所说的一样。GROUP之后得到了这样的数据： grunt&gt; DUMP B;((1,2,3),{(a,1,2,3,4.2,9.8,100),(a,1,2,3,1.4,0.2,500)})((1,2,5),{(a,1,2,5,7.7,5.9,200)})((3,0,5),{(a,3,0,5,3.5,2.1,200)})((7,9,9),{(b,7,9,9,,,300),(a,7,9,9,2.6,6.2,300)}) 其实到这里，我们肉眼就可以看出来最后要求的结果是什么了，当然，必须要由pig代码来完成，要不然怎么应对海量数据？文章来源：http://www.codelast.com/③这里的 FOREACH 与前面有点不一样，这就是所谓的&ldquo;嵌套的FOREACH&rdquo;。第一次看到这种写法，肯定会觉得很奇怪。先看一下用于去重的DISTINCT关键字的说明： Removes duplicate tuples in a relation. 然后再解释一下：FOREACH 是对B的每一行进行遍历，其中，B的每一行里含有一个包（bag），每一个包中含有若干元组（tuple）A，因此，FOREACH 后面的大括号里的操作，其实是对所谓的&ldquo;内部包&rdquo;（inner bag）的操作（详情请参看FOREACH的说明），在这里，我们指定了对A的col7这一列进行去重，去重的结果被命名为D，然后再对D计数（COUNT），就得到了我们想要的结果。④输出结果数据，与前文所述的差不多。这样就达成了我们的目的。从总体上说，刚接触pig不久的人会觉得这些写法怪怪的，就是扭不过来，但是要坚持，时间长了，连倒影也会让你觉得是正的了。 （8）如何将关系（relation）转换为标量（scalar）在前文中，我们要统计符合某些条件的数据的条数，使用了COUNT函数来计算，但在COUNT之后，我们得到的还是一个关系（relation），而不是一个标量的数字，如何把一个关系转换为标量，从而可以在后续处理中便于使用呢？具体请看这个链接。 （9）pig中如何使用shell进行辅助数据处理pig中可以嵌套使用shell进行辅助处理，下面，就以一个实际的例子来说明。假设我们在某一步pig处理后，得到了类似于下面 b.txt 中的数据： [root@localhost pig]$ cat b.txt1 5 98 = 734 8 6 3 262 0 6 = 65 问题：如何将数据中第4列中的&ldquo;=&rdquo;符号全部替换为9999？pig代码及输出结果如下： grunt&gt; A = LOAD 'b.txt' AS (col1:int, col2:int, col3:int, col4:chararray, col5:int);grunt&gt; B = STREAM A THROUGH awk &amp;#39;{if($4 == &amp;quot;=&amp;quot;) print $1&amp;quot;\\t&amp;quot;$2&amp;quot;\\t&amp;quot;$3&amp;quot;\\t9999\\t&amp;quot;$5; else print $0}&amp;#39;;grunt&gt; DUMP B;(1,5,98,9999,7)(34,8,6,3,2)(62,0,6,9999,65) 我们来看看这段代码是如何做到的：①加载数据，这个没什么好说的。②通过&ldquo;STREAM &hellip; THROUGH &hellip;&rdquo;的方式，我们可以调用一个shell语句，用该shell语句对A的每一行数据进行处理。此处的shell逻辑为：当某一行数据的第4列为&ldquo;=&rdquo;符号时，将其替换为&ldquo;9999&rdquo;；否则就照原样输出这一行。③输出B，可见结果正确。 （10）向pig脚本中传入参数假设你的pig脚本输出的文件是通过外部参数指定的，则此参数不能写死，需要传入。在pig中，使用传入的参数如下所示： STORE A INTO '$output_dir'; 则这个&ldquo;output_dir&rdquo;就是个传入的参数。在调用这个pig脚本的shell脚本中，我们可以这样传入参数： pig -param output_dir=&quot;/home/my_ourput_dir/&quot; my_pig_script.pig 这里传入的参数&ldquo;output_dir&rdquo;的值为&ldquo;/home/my_output_dir/&rdquo;。文章来源：http://www.codelast.com/（11）就算是同样一段pig代码，多次计算所得的结果也有可能是不同的例如用AVG函数来计算平均值时，同样一段pig代码，多次计算所得的结果中，小数点的最后几位也有可能是不相同的（当然也有可能相同），大概是因为精度的原因吧。不过，一般来说小数点的最后几位已经不重要了。例如我对一个数据集进行处理后，小数点后13位才开始有不同，这样的精度完全足够了。 （12）如何编写及使用自定义函数（UDF）请看这个链接：《Apache Pig中文教程（进阶）》 （13）什么是聚合函数（Aggregate Function）在pig中，聚合函数就是那些接受一个输入包（bag），返回一个标量（scalar）值的函数。COUNT函数就是一个例子。 （14）COGROUP做了什么与GROUP操作符一样，COGROUP也是用来分组的，不同的是，COGROUP可以按多个关系中的字段进行分组。还是以一个实例来说明，假设有以下两个数据文件： [root@localhost pig]$ cat a.txtuidk 12 3hfd 132 99bbN 463 231UFD 13 10 [root@localhost pig]$ cat b.txt908 uidk 888345 hfd 55728790 re 00000 现在我们用pig做如下操作及得到的结果为： grunt&gt; A = LOAD 'a.txt' AS (acol1:chararray, acol2:int, acol3:int);grunt&gt; B = LOAD 'b.txt' AS (bcol1:int, bcol2:chararray, bcol3:int);grunt&gt; C = COGROUP A BY acol1, B BY bcol2;grunt&gt; DUMP C;(re,{},{(28790,re,0)})(UFD,{(UFD,13,10)},{})(bbN,{(bbN,463,231)},{})(hfd,{(hfd,132,99)},{(345,hfd,557)})(uidk,{(uidk,12,3)},{(908,uidk,888)}) 每一行输出的第一项都是分组的key，第二项和第三项分别都是一个包（bag），其中，第二项是根据前面的key找到的A中的数据包，第三项是根据前面的key找到的B中的数据包。来看看第一行输出：&ldquo;re&rdquo;作为group的key时，其找不到对应的A中的数据，因此第二项就是一个空的包&ldquo;{}&rdquo;，&ldquo;re&rdquo;这个key在B中找到了对应的数据（28790 &nbsp; &nbsp;re &nbsp; &nbsp;00000），因此第三项就是包{(28790,re,0)}。其他输出数据也类似。 （15）安装pig后，运行pig命令时提示&ldquo;Cannot find hadoop configurations in classpath&rdquo;等错误的解决办法pig安装好后，运行pig命令时提示以下错误： ERROR org.apache.pig.Main - ERROR 4010: Cannot find hadoop configurations in classpath (neither hadoop-site.xml nor core-site.xml was found in the classpath).If you plan to use local mode, please put -x local option in command line 显而易见，提示找不到与hadoop相关的配置文件。所以我们需要把hadoop安装目录下的&ldquo;conf&rdquo;子目录添加到系统环境变量PATH中：修改 /etc/profile 文件，添加： export HADOOP_HOME=/usr/local/hadoopexport PIG_CLASSPATH=$HADOOP_HOME/conf PATH=$JAVA_HOME/bin:$HADOOP_HOME/bin:$PIG_CLASSPATH:$PATH 然后重新加载 /etc/profile 文件： source /etc/profile 文章来源：http://www.codelast.com/（16）piggybank是什么东西 Pig also hosts a UDF repository called piggybank that allows users to share UDFs that they have written. 说白了就是Apache把大家写的自定义函数放在一块儿，起了个名字，就叫做piggybank。你可以把它理解为一个SVN代码仓库。具体请看这里。 （17）UDF的构造函数会被调用几次你可能会想在UDF的构造函数中做一些初始化的工作，例如创建一些文件，等等。但是你不能假设UDF的构造函数只被调用一次，因此，如果你要在构造函数中做一些只能做一次的工作，你就要当心了&mdash;&mdash;可能会导致错误。 （18）LOAD数据时，如何一次LOAD多个目录下的数据例如，我要LOAD两个HDFS目录下的数据：/abc/2010 和 /abc/2011，则我们可以这样写LOAD语句： A = LOAD '/abc/201{0,1}'; （19）怎样自己写一个UDF中的加载函数(load function)请看这个链接：《Apache Pig中文教程（进阶）》 （20）重载(overloading)一个UDF请看这个链接：《Apache Pig中文教程（进阶）》。 （21）pig运行不起来，提示&ldquo;org.apache.hadoop.ipc.Client - Retrying connect to server:&nbsp;请看这个链接：《Apache Pig中文教程（进阶）》 （22）用含有null的字段来GROUP，结果会如何假设有数据文件 a.txt 内容为： 1 2 51 31 36 9 8 其中，每两列数据之间是用tab分割的，第二行的第2列、第三行的第3列没有内容（也就是说，加载到Pig里之后，对应的数据会变成null），如果把这些数据按第1、第2列来GROUP的话，第1、2列中含有null的行会被忽略吗？来做一下试验： A = LOAD 'a.txt' AS (col1:int, col2:int, col3:int);B = GROUP A BY (col1, col2);DUMP B; 输出结果为： ((1,2),{(1,2,5)})((1,3),{(1,3,)})((1,),{(1,,3)})((6,9),{(6,9,8)}) 从上面的结果（第三行）可见，原数据中第1、2列里含有null的行也被计入在内了，也就是说，GROUP操作是不会忽略null的，这与COUNT有所不同（见本文前面的部分）。 （23）如何统计数据中某些字段的组合有多少种假设有如下数据： [root@localhost]# cat a.txt1 3 4 71 3 5 42 7 0 59 8 6 6 现在我们要统计第1、2列的不同组合有多少种，对本例来说，组合有三种： 1 32 79 8 也就是说我们要的答案是3。用Pig怎么计算？文章来源：http://www.codelast.com/先写出全部的Pig代码： A = LOAD 'a.txt' AS (col1:int, col2:int, col3:int, col4:int);B = GROUP A BY (col1, col2);C = GROUP B ALL;D = FOREACH C GENERATE COUNT(B);DUMP D; 然后再来看看这些代码是如何计算出上面的结果的：①第一行代码加载数据，没什么好说的。②第二行代码，得到第1、2列数据的所有组合。B的数据结构为： grunt&gt; DESCRIBE B;B: {group: (col1: int,col2: int),A: {col1: int,col2: int,col3: int,col4: int}} 把B DUMP出来，得到： ((1,3),{(1,3,4,7),(1,3,5,4)})((2,7),{(2,7,0,5)})((9,8),{(9,8,6,6)}) 非常明显，(1,3)，(2,7)，(9,8)的所有组合已经被排列出来了，这里得到了若干行数据。下一步我们要做的就是统计这样的数据一共有多少行，也就得到了第1、2列的组合有多少组。③第三和第四行代码，就实现了统计数据行数的功能。参考本文前面部分的&ldquo;怎样统计数据行数&rdquo;一节。就明白这两句代码是什么意思了。这里需要特别说明的是：a)为什么倒数第二句代码中是COUNT(B)，而不是COUNT(group)？我们是对C进行FOREACH，所以要先看看C的数据结构： grunt&gt; DESCRIBE C;C: {group: chararray,B: {group: (col1: int,col2: int),A: {col1: int,col2: int,col3: int,col4: int}}} 可见，你可以把C想像成一个map的结构，key是一个group，value是一个包（bag），它的名字是B，这个包中有N个元素，每一个元素都对应到②中所说的一行。根据②的分析，我们就是要统计B中元素的个数，因此，这里当然就是COUNT(B)了。b)COUNT函数的作用是统计一个包（bag）中的元素的个数： COUNT Computes the number of elements in a bag. 从C的数据结构看，B是一个bag，所以COUNT函数是可以用于它的。如果你试图把COUNT应用于一个非bag的数据结构上，会发生错误，例如： java.lang.ClassCastException: org.apache.pig.data.BinSedesTuple cannot be cast to org.apache.pig.data.DataBag 这是把Tuple传给COUNT函数时发生的错误。 （24）两个整型数相除，如何转换为浮点型，从而得到正确的结果这个问题其实很傻，或许不用说你也知道了：假设有int a = 3 和 int b = 2两个数，在大多数编程语言里，a/b得到的是1，想得到正确结果1.5的话，需要转换为float再计算。在Pig中其实和这种情况一样，下面就拿几行数据来做个实验： [root@localhost ~]# cat a.txt3 24 5 在Pig中： grunt&gt; A = LOAD 'a.txt' AS (col1:int, col2:int);grunt&gt; B = FOREACH A GENERATE col1/col2;grunt&gt; DUMP B;(1)(0) 可见，不加类型转换的计算结果是取整之后的值。那么，转换一下试试： grunt&gt; A = LOAD 'a.txt' AS (col1:int, col2:int);grunt&gt; B = FOREACH A GENERATE (float)(col1/col2);grunt&gt; DUMP B;(1.0)(0.0) 这样转换还是不行的，这与大多数编程语言的结果一致&mdash;&mdash;它只是把取整之后的数再转换为浮点数，因此当然是不行的。文章来源：http://www.codelast.com/正确的做法应该是： grunt&gt; A = LOAD 'a.txt' AS (col1:int, col2:int);grunt&gt; B = FOREACH A GENERATE (float)col1/col2;grunt&gt; DUMP B;(1.5)(0.8) 或者这样也行： grunt&gt; A = LOAD 'a.txt' AS (col1:int, col2:int);grunt&gt; B = FOREACH A GENERATE col1/(float)col2;grunt&gt; DUMP B;(1.5)(0.8) 这与我们的通常做法是一致的，因此，你要做除法运算的时候，需要注意这一点。 （25）UNION的一个例子假设有两个数据文件为： [root@localhost ~]# cat 1.txt0 31 50 8 [root@localhost ~]# cat 2.txt1 60 9 现在要求出：在第一列相同的情况下，第二列的和分别为多少？例如，第一列为 1 的时候，第二列有5和6两个值，和为11。同理，第一列为0的时候，第二列的和为 3+8+9=20。计算此问题的Pig代码如下： A = LOAD '1.txt' AS (a: int, b: int);B = LOAD '2.txt' AS (c: int, d: int);C = UNION A, B;D = GROUP C BY $0;E = FOREACH D GENERATE FLATTEN(group), SUM(C.$1);DUMP E; 输出为： (0,20)(1,11) 文章来源：http://www.codelast.com/我们来看看每一步分别做了什么：①第1行、第2行代码分别加载数据到关系A、B中，没什么好说的。②第3行代码，将关系A、B合并起来了。合并后的数据结构为： grunt&gt; DESCRIBE C;C: {a: int,b: int} 其数据为： grunt&gt; DUMP C;(0,3)(1,5)(0,8)(1,6)(0,9) ③第4行代码按第1列（即$0）进行分组，分组后的数据结构为： grunt&gt; DESCRIBE D;D: {group: int,C: {a: int,b: int}} 其数据为： grunt&gt; DUMP D;(0,{(0,9),(0,3),(0,8)})(1,{(1,5),(1,6)}) ④最后一行代码，遍历D，将D中每一行里的所有bag(即C)的第2列(即$1)进行累加，就得到了我们要的结果。 （26）错误&ldquo;ERROR org.apache.pig.tools.grunt.Grunt - ERROR 2042: Error in new logical plan. Try -Dpig.usenewlogicalplan=false.&rdquo;的可能原因①Pig的bug，详见此链接；②其他原因。我遇到并解决了一例。具体的代码不便在此陈列，但是基本可以说是由于自己写的Pig代码对复杂数据结构的处理不当导致的，后来我尝试更改了一种实现方式，就绕过了这个问题。关于这点，确实还是要具体问题具体分析的，在这里没有实例的话，无法给大家一个明确的解决问题的指南。 （27）如何在Pig中使用正则表达式对字符串进行匹配假设你有如下数据文件： [root@localhost ~]# cat a.txt1 http://ui.qq.com/abcd.html5 http://tr.qq.com/743.html8 http://vid.163.com/trees.php9 http:auto.qq.com/us.php 现在要找出该文件中，第二列符合&ldquo;//.qq.com/*&rdquo;模式的所有行（此处只有前两行符合条件），怎么做？Pig代码如下： A = LOAD 'a.txt' AS (col1: int, col2: chararray);B = FILTER A BY col2 matches './/..qq.com/.*';DUMP B; 我们看到，matches关键字对 col2 进行了正则匹配，它使用的是Java格式的正则表达式匹配规则。.&nbsp;表示任意字符，*&nbsp;表示字符出现任意次数；.&nbsp;对&nbsp;.&nbsp;进行了转义，表示匹配&nbsp;.&nbsp;这个字符；/&nbsp;就是表示匹配&nbsp;/&nbsp;这个字符。这里需要注意的是，在引号中，用于转义的字符&nbsp;\\&nbsp;需要打两个才能表示一个，所以上面的&nbsp;\\.&nbsp;就是与正则中的&nbsp;.&nbsp;是一样的，即匹配&nbsp;.&nbsp;这个字符。所以，如果你要匹配数字的话，应该用这种写法（\\d表示匹配数字，在引号中必须用\\d）： B = FILTER A BY (col matches '\\d.*'); 文章来源：http://www.codelast.com/最后输出结果为： (1,http://ui.qq.com/abcd.html)(5,http://tr.qq.com/743.html) 可见结果是正确的。 （28）如何截取一个字符串中的某一段在处理数据时，如果你想提取出一个日期字符串的年份，例如提取出&ldquo;2011-10-26&rdquo;中的&ldquo;2011&rdquo;，可以用内置函数 SUBSTRING 来实现： SUBSTRING Returns a substring from a given string. Syntax SUBSTRING(string, startIndex, stopIndex) 下面举一个例子。假设有数据文件： [root@localhost ~]# cat a.txt2010-05-06 abc2008-06-18 uio2011-10-11 tyr2010-12-23 fgh2011-01-05 vbn 第一列是日期，现在要找出所有不重复的年份有哪些，可以这样做： A = LOAD 'a.txt' AS (dateStr: chararray, flag: chararray);B = FOREACH A GENERATE SUBSTRING(dateStr, 0, 4);C = DISTINCT B;DUMP C; 输出结果为： (2008)(2010)(2011) 可见达到了我们想要的效果。上面的代码太简单了，不必多言，唯一需要说明一下的是 SUBSTRING 函数，它的第一个参数是要截取的字符串，第二个参数是起始索引（从0开始），第三个参数是结束索引。文章来源：http://www.codelast.com/（29）如何拼接两个字符串假设有以下数据文件： [root@localhost ~]# cat 1.txtabc 123cde 456fgh 789ijk 200 现在要把第一列和第二列作为字符串拼接起来，例如第一行会变成&ldquo;abc123&rdquo;，那么使用CONCAT这个求值函数（eval function）就可以做到： A = LOAD '1.txt' AS (col1: chararray, col2: int);B = FOREACH A GENERATE CONCAT(col1, (chararray)col2);DUMP B; 输出结果为： (abc123)(cde456)(fgh789)(ijk200) 注意这里故意在加载数据的时候把第二列指定为int类型，这是为了说明数据类型不一致的时候CONCAT会出错（你可以试验一下）： ERROR org.apache.pig.tools.grunt.Grunt - ERROR 1045: Could not infer the matching function for org.apache.pig.builtin.CONCAT as multiple or none of them fit. Please use an explicit cast. 所以在后面CONCAT的时候，对第二列进行了类型转换。另外，如果数据文件内容为： [root@localhost ~]# cat 1.txt5 1237 4568 7890 200 那么，如果对两列整数CONCAT： A = LOAD '1.txt' AS (col1: int, col2: int);B = FOREACH A GENERATE CONCAT(col1, col2); 同样也会出错： ERROR org.apache.pig.tools.grunt.Grunt - ERROR 1045: Could not infer the matching function for org.apache.pig.builtin.CONCAT as multiple or none of them fit. Please use an explicit cast. 要注意这一点。有人可能会问：要拼接几个字符串的话怎么办？CONCAT 套 CONCAT 就要可以了（有点笨，但管用）： CONCAT(a, CONCAT(b, c)) （30）如何求两个数据集的重合 &amp; 不同的数据类型JOIN会失败假设有以下两个数据文件： [root@localhost ~]# cat 1.txt123456789200 以及： [root@localhost ~]# cat 2.txt200333789 现在要找出两个文件中，相同的数据有多少行，怎么做？这也就是所谓的求两个数据集的重合。用关系操作符JOIN，我们可以达到这个目的。在处理海量数据时，经常会有求重合的需求。所以JOIN是Pig中一个极其重要的操作。在本例中，两个文件中有两个相同的数据行：789以及200，因此，结果应该是2。我们先来看看正确的代码： A = LOAD '1.txt' AS (a: int);B = LOAD '2.txt' AS (b: int);C = JOIN A BY a, B BY b;D = GROUP C ALL;E = FOREACH D GENERATE COUNT(C);DUMP E; 解释一下：①第一、二行是加载数据，不必多言。②第三行按A的第1列、B的第二列进行&ldquo;结合&rdquo;，JOIN之后，a、b两列不相同的数据就被剔除掉了。C的数据结构为： C: {A::a: int,B::b: int} C的数据为： (200,200)(789,789) ③由于我们要统计的是数据行数，所以上面的Pig代码中的第4、5行就进行了计数的运算。④如果文件 2.txt 多一行数据&ldquo;200&rdquo;，结果会是什么？答案是：结果为3行。这个时候C的数据为： (200,200)(200,200)(789,789) 所以如果你要去除重复的，还需要用DISTINCE对C处理一下： A = LOAD '1.txt' AS (a: int);B = LOAD '2.txt' AS (b: int);C = JOIN A BY a, B BY b;uniq_C = DISTINCT C;D = GROUP uniq_C ALL;E = FOREACH D GENERATE COUNT(uniq_C);DUMP E; 这样得到的结果就是2了。文章来源：http://www.codelast.com/尤其需要注意的是，如果JOIN的两列具有不同的数据类型，是会失败的。例如以下代码： A = LOAD '1.txt' AS (a: int);B = LOAD '2.txt' AS (b: chararray);C = JOIN A BY a, B BY b;D = GROUP C ALL;E = FOREACH D GENERATE COUNT(C);DUMP E; 在语法上是没有错误的，但是一运行就会报错： ERROR org.apache.pig.tools.grunt.Grunt - ERROR 1107: Cannot merge join keys, incompatible types 这是因为a、b具有不同的类型：int和chararray。 （31）使用三目运算符&ldquo; ? : &rdquo;有时候必须加括号假设有以下数据文件： [root@localhost ~]# cat a.txt5 8 96 04 3 1 其中，第二行的第二列数据是有缺失的，因此，加载数据之后，它会成为null。顺便废话一句，在处理海量数据时，数据有缺失是经常遇到的现象。现在，我们如果要把所有缺失的数据填为 -1， 可以使用三目运算符来操作： A = LOAD 'a.txt' AS (col1:int, col2:int, col3:int);B = FOREACH A GENERATE col1, ((col2 is null)? -1 : col2), col3;DUMP B; 输出结果为： (5,8,9)(6,-1,0)(4,3,1) ((col2 is null)? -1 : col2) 的含义不用解释你也知道，就是当col2为null的时候将其置为-1，否则就保持原来的值，但是注意，它最外面是用括号括起来的，如果去掉括号，写成&nbsp;(col2 is null)? -1 : col2，那么就会有语法错误： ERROR org.apache.pig.tools.grunt.Grunt - ERROR 1000: Error during parsing. Encountered &quot; &quot;is&quot; &quot;is &quot;&quot; at line 1, column 36. Was expecting one of （后面省略） 错误提示有点不直观。所以，有时候使用三目运算符是必须要使用括号的。 （32）如何补上缺失的数据通过前面的文章，我们已经知道了如何按自己的需求补上缺失的数据，那么这里还有一个例子，可以让你多了解一些特殊的情况。数据文件如下： [root@localhost ~]# cat 1.txt1 (4,9)58 (3,0)5 (9,2)6 这些数据的布局比较怪，我们要把它加载成什么样的schema呢？答：第一列为一个int，第二列为一个tuple，此tuple又含两个int。加载成这样的模式不是为了制造复杂度，而是为了说明后面的问题而设计的。同时，我们也注意到，第二列数据是有缺失的。问题：怎样求在第一列数据相同的情况下，第二列数据中的第一个整数的和分别为多少？例如，第一列为1的数据只有一行（即第一行），因此，第二列的第一个整数的和就是4。但是对最后一行，也就是第一列为6时，由于其第二列数据缺失，我们希望它输出的结果是0。先来看看Pig代码： A = LOAD '1.txt' AS (a:int, b:tuple(x:int, y:int));B = FOREACH A GENERATE a, FLATTEN(b);C = GROUP B BY a;D = FOREACH C GENERATE group, SUM(B.x);DUMP D; 结果为： (1,4)(5,9)(6,)(8,3) 我们注意到，(5,9) 这一行是由数据文件 1.txt 的第 2、4行计算得到的，其中，第2行数据有缺失，但这并不影响求和计算，因为另一行数据没有缺失。你可以这样想：一个包（bag）中有多个数，当其中一个为null，而其他不为null时，把它们相加会自动忽略null。然而，第三行 (6,) 是不是太刺眼了？没错，因为数据文件 1.txt 的最后一行缺失了第二列，所以，在 SUM(B.x) 中的 B.x 为null就会导致计算结果为null，从而什么也输出不了。这就与我们期望的输出有点不同了。我们希望这种缺失的数据不要空着，而是输出0。该怎么做呢？文章来源：http://www.codelast.com/想法1： D = FOREACH C GENERATE group, ((IsEmpty(B.x)) ? 0 : SUM(B.x)); 输出结果为： (1,4)(5,9)(6,)(8,3) 可见行不通。从这个结果我们知道，IsEmpty(B.x) 为false，即B.x不是empty的，所以不能这样做。想法2： D = FOREACH C GENERATE group, ((B.x is null) ? 0 : SUM(B.x)); 输出结果还是与上面一样！仍然行不通。这更奇怪了：B.x既非empty，也非null，那么它是什么情况？按照我的理解，当group为6时，它应该是一个非空的包（bag），里面有一个null的东西，所以，这个包不是empty的，它也非null。我不知道这样理解是否正确，但是它看上去就像是这样的。想法3： D = FOREACH C GENERATE group, SUM(B.x) AS s;E = FOREACH D GENERATE group, ((s is null) ? -1 : s);DUMP E; 输出结果为： (1,4)(5,9)(6,-1)(8,3) 可见达到了我们想要的结果。这与本文前面部分的做法是一致的，即：先得到含null的结果，再把这个结果中的null替换为指定的值。有人会问：就没有办法在生成数据集D的时候，就直接通过判断语句来实现这个效果吗？据我目前所知是不行的，如果哪位读者知道，不妨告知。 （33）DISTINCT操作用于去重，正因为它要把数据集合到一起，才知道哪些数据是重复的，因此，它会产生reduce过程。同时，在map阶段，它也会利用combiner来先去除一部分重复数据以加快处理速度。 （34）如何将Pig job的优先级设为HIGH嫌Pig job运行太慢？只需在Pig脚本的开头加上一句： set job.priority HIGH; 即可将Pig job的优先级设为高了。 （35）&ldquo;Scalars can be only used with projections&rdquo;错误的原因这个错误提示比较不直观，光看这句话是不容易发现错误所在的，但是，只要你一Google，可能就找到原因了，例如这个链接里的反馈。在这里，我也想用一个简单的例子给大家用演示一下产生这个错误的原因之一。假设有如下数据文件： [root@localhost ~]$ cat 1.txta 1b 8c 3c 3d 6d 3c 5e 7 现在要统计：在第1列的每一种组合下，第二列为3和6的数据分别有多少条？例如，当第1列为 c 时，第二列为3的数据有2条，为6的数据有0条；当第1列为d时，第二列为3的数据有1条，为6的数据有1条。其他的依此类推。Pig代码如下： A = LOAD '1.txt' AS (col1:chararray, col2:int);B = GROUP A BY col1;C = FOREACH B {D = FILTER A BY col2 == 3;E = FILTER A BY col2 == 6;GENERATE group, COUNT(D), COUNT(E);};DUMP C; 输出结果为： (a,0,0)(b,0,0)(c,2,0)(d,1,1)(e,0,0) 可见结果是正确的。文章来源：http://www.codelast.com/那么，如果我在上面的代码中，把&ldquo;D = FILTER A BY col2 == 3&rdquo;不小心写成了&ldquo;D = FILTER B BY col2 == 3&rdquo;，就肯定会得到&ldquo;Scalars can be only used with projections&rdquo;的错误提示。说白了，还是要时刻注意你每一步生成的数据的结构，眼睛睁大，千万不要用错了relation。 （36）什么是嵌套的FOREACH/内部的FOREACH嵌套的（nested）FOREACH和内部的（inner）FOREACH是一个意思，正如你在本文第(35)条中所见，一个FOREACH可以对每一条记录施以多种不同的关系操作，然后再GENERATE得到想要的结果，这就是嵌套的/内部的FOREACH。 （37）错误&ldquo;Could not infer the matching function for org.apache.pig.builtin.CONCAT&rdquo;的原因之一如果你遇到这个错误，那么有可能是你在多级CONCAT嵌套的时候，没有写对语句，例如&ldquo;CONCAT(CONCAT(CONCAT(a, b), c), d)&rdquo;这样的嵌套，由于括号众多，所以写错了是一点也不奇怪的。我遇这个错误的时候，是由于CONCAT太多，自己多写了一个都没有发现。希望我的提醒能给你一点解决问题的提示。 （38）用Pig加载HBase数据时遇到的错误&ldquo;ERROR 2999: Unexpected internal error. could not instantiate 'com.twitter.elephantbird.pig.load.HBaseLoader' with arguments XXX&rdquo;的原因之一请看这个链接：《Apache Pig中文教程（进阶）》 （39）错误&ldquo;ERROR 1039: In alias XX, incompatible types in EqualTo Operator left hand side:XXX right hand side:XXX&rdquo;的原因其实这个错误提示太明显了，就是类型不匹配造成的。上面的XXX可以指代不同的类型。这说明，前面可能有一个类型为long的字段，后面你却把它当chararray来用了，例如： A = LOAD '1.txt' AS (col1: int, col2: long);B = FILTER A BY col2 == '123456789';C = GROUP B ALL;D = FOREACH C GENERATE COUNT(B);DUMP D; 就会出错： ERROR 1039: In alias B, incompatible types in EqualTo Operator left hand side:long right hand side:chararray 只要把col2强制类型转换一下（或者一开始就将其类型指定为chararray）就可以解决问题。文章来源：http://www.codelast.com/不仅在进行数据比较中，在JOIN时也经常出现数据类型不匹配导致的错误问题。我在实际工作中发现，有的同学写了比较长的Pig代码，出现了这样的错误却不会仔细去看错误提示，而是绞尽脑汁地逐句去检查语法（语法是没有错的），结果费了很大的劲才知道是类型问题，得不偿失，还不如仔细看错误提示想想为什么。 （40）在grunt交互模式下，如何在编辑Pig代码的时候跳到行首和行末/行尾在grunt模式下，如果你写了一句超长的Pig代码，那么，你想通过HOME/END键跳到行首和行末是做不到的。按HOME时，Pig会在你的光标处插入一个&ldquo;1~&rdquo;： A = LOAD '1.txt' AS (col: int1~); 按END时，Pig会在你的光标处插入一个&ldquo;4~&rdquo;： A = LOAD '1.txt' AS (col: int4~); 正确的做法是：按Ctrl+A 和 Ctrl+E 代替 HOME 和 END，就可以跳到行首和行末了。 （41）不能对同一个关系（relation）进行JOIN假设有如下文件： [root@localhost ~]# cat 1.txt1 a2 e3 v4 n 我想对第一列这样JOIN： A = LOAD '1.txt' AS (col1: int, col2: chararray);B = JOIN A BY col1, A BY col1; 那么当你试图 DUMP B 的时候，会报如下的错： ERROR org.apache.pig.tools.grunt.Grunt - ERROR 1108: Duplicate schema alias: A::col1 in &quot;B&quot; 这是因为Pig会弄不清JOIN之后的字段名&mdash;&mdash;两个字段均为A::col1，使得一个关系（relation）中出现了重复的名字，这是不允许的。文章来源：http://www.codelast.com/要解决这个问题，只需将数据LOAD两次，并且给它们起不同的名字就可以了： grunt&gt; A = LOAD '1.txt' AS (col1: int, col2: chararray);grunt&gt; B = LOAD '1.txt' AS (col1: int, col2: chararray);grunt&gt; C = JOIN A BY col1, B BY col1;grunt&gt; DESCRIBE C;C: {A::col1: int,A::col2: chararray,B::col1: int,B::col2: chararray}grunt&gt; DUMP C;(1,a,1,a)(2,e,2,e)(3,v,3,v)(4,n,4,n) 从上面的 C 的schema，你可以看出来，如果对同一个关系A的第一列进行JOIN，会导致schema中出现相同的字段名，所以当然会出错。 （42）外部的JOIN(outer JOIN)初次使用JOIN时，一般人使用的都是所谓的&ldquo;内部的JOIN&rdquo;(inner JOIN)，也即类似于 C = JOIN A BY col1, B BY col2 这样的JOIN。Pig也支持&ldquo;外部的JOIN&rdquo;(outer JOIN)，下面就举一个例子。假设有文件： [root@localhost ~]# cat 1.txt1 a2 e3 v4 n 以及： [root@localhost ~]# cat 2.txt9 a2 e3 v0 n 现在来对这两个文件的第一列作一个outer JOIN： grunt&gt; A = LOAD '1.txt' AS (col1: int, col2: chararray);grunt&gt; B = LOAD '2.txt' AS (col1: int, col2: chararray);grunt&gt; C = JOIN A BY col1 LEFT OUTER, B BY col1;grunt&gt; DESCRIBE C;C: {A::col1: int,A::col2: chararray,B::col1: int,B::col2: chararray}grunt&gt; DUMP C;(1,a,,)(2,e,2,e)(3,v,3,v)(4,n,,) 在outer JOIN中，&ldquo;OUTER&rdquo;关键字是可以省略的。从上面的结果，我们注意到：如果换成一个inner JOIN，则两个输入文件的第一、第四行都不会出现在结果中（因为它们的第一列不相同），而在LEFT OUTER JOIN中，文件1.txt的第一、四行却被输出了，所以这就是LEFT OUTER JOIN的特点：对左边的记录来说，即使它与右边的记录不匹配，它也会被包含在输出数据中。文章来源：http://www.codelast.com/同理可知RIGHT OUTER JOIN的功能&mdash;&mdash;把上面的 LEFT 换成 RIGHT，结果如下： (,,0,n)(2,e,2,e)(3,v,3,v)(,,9,a) 可见，与左边的记录不匹配的右边的记录被保存了下来，而左边的记录没有保存下来（两个逗号表明其为空），这就是RIGHT OUTER JOIN的效果，与我们想像的一样。有人会问，OUTER JOIN在实际中可以用来做什么？举一个例子：可以用来求&ldquo;不在某数据集中的那些数据（即：不重合的数据）&rdquo;。还是以上面的两个数据文件为例，现在我要求出 1.txt 中，第一列不在 2.txt 中的第一列的那些记录，肉眼一看就知道，1和4这两个数字在 2.txt 的第一列里没有出现，而2和3出现了，因此，我们要找的记录就是： 1 a4 n 要实现这个效果，Pig代码及结果为： grunt&gt; A = LOAD '1.txt' AS (col1: int, col2: chararray);grunt&gt; B = LOAD '2.txt' AS (col1: int, col2: chararray);grunt&gt; C = JOIN A BY col1 LEFT OUTER, B BY col1;grunt&gt; DESCRIBE C;C: {A::col1: int,A::col2: chararray,B::col1: int,B::col2: chararray}grunt&gt; D = FILTER C BY (B::col1 is null);grunt&gt; E = FOREACH D GENERATE A::col1 AS col1, A::col2 AS col2;grunt&gt; DUMP E;(1,a)(4,n) 可见，我们确实找出了&ldquo;不重合的记录&rdquo;。在作海量数据分析时，这种功能是极为有用的。最后来一个总结：假设有两个数据集（在1.txt和2.txt中），分别都只有1列，则如下代码： A = LOAD '1.txt' AS (col1: chararray);B = LOAD '2.txt' AS (col1: chararray);C = JOIN A BY col1 LEFT OUTER, B BY col1;D = FILTER C BY (B::col1 is null);E = FOREACH D GENERATE A::col1 AS col1;DUMP E; 计算结果为：在A中，但不在B中的记录。 （43）JOIN的优化请看这个链接：《Apache Pig中文教程（进阶）》 （44）GROUP时按所有字段分组可以用GROUP ALL吗假设你有如下数据文件： [root@localhost ~]# cat 3.txt1 92 23 34 01 91 94 0 现在要找出第1、2列的组合中，每一种的个数分别为多少，例如，(1,9)组合有3个，(4,0)组合有两个，依此类推。显而易见，我们只需要用GROUP就可以轻易完成这个任务。于是写出如下代码： A = LOAD '3.txt' AS (col1: int, col2: int);B = GROUP A ALL;C = FOREACH B GENERATE group, COUNT(A);DUMP C; 可惜，结果不是我们想要的： (all,7) 为什么呢？我们的本意是按所有列来GROUP，于是使用了GROUP ALL，但是这实际上变成了统计行数，下面的代码就是一段标准的统计数据行数的代码： A = LOAD '3.txt' AS (col1: int, col2: int);B = GROUP A ALL;C = FOREACH B GENERATE COUNT(A);DUMP C; 因此，上面的&nbsp;C = FOREACH B GENERATE group, COUNT(A) 也无非就是多打印了一个group的名字（all）而已&mdash;&mdash;group的名字被设置为&ldquo;all&rdquo;，这是Pig帮你做的。文章来源：http://www.codelast.com/正确的做法很简单，只需要按所有字段GROUP，就可以了： A = LOAD '3.txt' AS (col1: int, col2: int);B = GROUP A BY (col1, col2);C = FOREACH B GENERATE group, COUNT(A);DUMP C; 结果如下： ((1,9),3)((2,2),1)((3,3),1)((4,0),2) 这与我们前面分析的正确结果是一样的。 （45）在Pig中使用中文字符串有读者来信问我，如何在Pig中使用中文作为FILTER的条件？我做了如下测试，结论是可以使用中文。数据文件 data.txt 内容为（每一列之间以TAB为分隔符）： 1 北京市 a2 上海市 b3 北京市 c4 北京市 f5 天津市 e Pig脚本文件 test.pig 内容为： A = LOAD 'data.txt' AS (col1: int, col2: chararray, col3: chararray);B = FILTER A BY (col2 == '北京市');DUMP B; 首先，我这两个文件的编码都是UTF-8(无BOM)，在Linux命令行下，我直接以本地模式执行Pig脚本 test.pig： pig -x local test.pig 得到的输出结果为： (1,北京市,a)(3,北京市,c)(4,北京市,f) 可见结果是正确的。文章来源：http://www.codelast.com/但是，如果我在grunt交互模式下，把 test.pig 的内容粘贴进去执行，是得不到任何输出结果的： grunt&gt; A = LOAD 'data.txt' AS (col1: int, col2: chararray, col3: chararray);grunt&gt; B = FILTER A BY (col2 == '北京市');grunt&gt; DUMP B; 具体原因我不清楚，但是至少有一点是肯定的：可以使用中文作为FILTER的条件，只要不在交互模式下执行你的Pig脚本即可。 （46）如何统计 tuple 中的 field 数，bag 中的 tuple 数，map 中的 key/value 组数一句话：用Pig内建的 SIZE 函数： Computes the number of elements based on any Pig data type. 具体可看这个链接。 （47）一个字符串为null，与它为空不一定等价在某些情况下，要获取&ldquo;不为空&rdquo;的字符串，仅仅用 is not null 来判断是不够的，还应该加上 SIZE(field_name) &gt; 0 的条件： B = FILTER A BY (field_name is not null AND (SIZE(field_name) &gt; 0L)); 注意，这只是在某些情况下需要这样做，在一般情况下，仅用 is not null 来过滤就可以了。我并没有总结出特殊情况是哪些情况，我只能说我我不是第一次遇到此情况了，所以才有了这一个结论。注意上面使用的是&ldquo;0L&rdquo;，因为SIZE()返回的是long类型，如果不加L，在Pig0.10下会出现一个警告，例如： [main] WARN &nbsp;org.apache.pig.PigServer - Encountered Warning IMPLICIT_CAST_TO_LONG 1 time(s) （48）Pig中的各operator（操作符），哪些会触发reduce过程①GROUP：由于GROUP操作会将所有具有相同key的记录收集到一起，所以数据如果正在map中处理的话，就会触发shuffle&rarr;reduce的过程。②ORDER：由于需要将所有相等的记录收集到一起（才能排序），所以ORDER会触发reduce过程。同时，除了你写的那个Pig job之外，Pig还会添加一个额外的M-R job到你的数据流程中，因为Pig需要对你的数据集做采样，以确定数据的分布情况，从而解决数据分布严重不均的情况下job效率过于低下的问题。③DISTINCT：由于需要将记录收集到一起，才能确定它们是不是重复的，因此DISTINCT会触发reduce过程。当然，DISTINCT也会利用combiner在map阶段就把重复的记录移除。④JOIN：JOIN用于求重合，由于求重合的时候，需要将具有相同key的记录收集到一起，因此，JOIN会触发reduce过程。⑤LIMIT：由于需要将记录收集到一起，才能统计出它返回的条数，因此，LIMIT会触发reduce过程。⑥COGROUP：与GROUP类似（参看本文前面的部分），因此它会触发reduce过程。 ⑦CROSS：计算两个或多个关系的叉积。 （49）如何统计一个字符串中包含的指定字符数这可以不算是个Pig的问题了，你可以把它认为是一个shell的问题。从本文前面部分我们已经知道，Pig中可以用 STREAM … THROUGH 来调用shell进行辅助数据处理，所以在这我们也能这样干。假设有文本文件： [root@localhost ~]$ cat 1.txt123 abcdef:243789174456 DFJKSDFJ:3646:555558888789 yKDSF:00000%0999:2343324:11111:33333 现在要统计：每一行中，第二列里所包含的冒号（&ldquo;:&rdquo;）分别为多少？代码如下： A = LOAD '1.txt' AS (col1: chararray, col2: chararray);B = STREAM A THROUGH awk -F&amp;quot;:&amp;quot; &amp;#39;{print NF-1}&amp;#39; AS (colon_count: int);DUMP B; 结果为： (1)(2)(4) 文章来源：http://www.codelast.com/（50）UDF是区分大小写的因为UDF是由Java类来实现的，所以区分大小写，就这么简单。 （51）设置Pig job的job name在Pig脚本开头加上一句： set job.name 'My-Job-Name'; 那么，执行该Pig脚本之后，在Hadoop的Job Tracker中看到的&ldquo;Name&rdquo;就是&ldquo;My-Job-Name&rdquo;了。如果不设置，显示的name是类似于&ldquo;Job6245768625829738970.jar&rdquo;这样的东西，job多的时候完全没有标识度，建议一定要设置一个特殊的job name。 （52）把纯文本转化为JSON假设输入文件 a.txt 内容为： 1 29 8 则如下Pig代码将把它转化为JSON格式： A = LOAD 'a.txt' AS (col1: chararray, col2: chararray);B = STORE A INTO 'result' USING JsonStorage(); 查看输出文件的内容是： {&quot;col1&quot;:&quot;1&quot;,&quot;col2&quot;:&quot;2&quot;}{&quot;col1&quot;:&quot;9&quot;,&quot;col2&quot;:&quot;8&quot;} 可见，你LOAD输入数据时定义的字段名，就是输出文件中的JSON字段名。 &nbsp; SyntaxHighlighter.defaults['class-name'] = ''; SyntaxHighlighter.defaults['smart-tabs'] = true; SyntaxHighlighter.defaults['tab-size'] = 2; SyntaxHighlighter.defaults['gutter'] = true; SyntaxHighlighter.defaults['quick-code'] = true; SyntaxHighlighter.defaults['collapse'] = false; SyntaxHighlighter.defaults['auto-links'] = true; SyntaxHighlighter.defaults['toolbar'] = true; SyntaxHighlighter.all();","link":"/2019/02/20/Pig的一些基础概念及用法总结-转/"}],"tags":[{"name":"Video","slug":"Video","link":"/tags/Video/"},{"name":"Qzone","slug":"Qzone","link":"/tags/Qzone/"},{"name":"Data Science","slug":"Data-Science","link":"/tags/Data-Science/"},{"name":"Essay","slug":"Essay","link":"/tags/Essay/"}],"categories":[]}